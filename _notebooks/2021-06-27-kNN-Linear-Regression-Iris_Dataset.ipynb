{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('hlu': conda)"
  },
  "interpreter": {
   "hash": "64ed01536bf604a35b39c8ffb5f8754e0194885370844d2a99bdfd2c5ccd0baa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# \"kNN & Linear Regression From Scratch: Iris Dataset\"\n",
    "> A step-by-step implementation of the k-Nearest Neighbours and Linear Regression algorithms using just the standard Python libaries.\n",
    "\n",
    "- toc: true\n",
    "- author: Oluwaleke Umar Yusuf\n",
    "- badges: true\n",
    "- comments: true\n",
    "- image: images/ipynb/iris_dataset.png\n",
    "- hide: false\n",
    "- search_exclude: false\n",
    "- categories: [notebook]\n",
    "- tags: [kNN, Iris Dataset]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# kNN & Linear Regression From Scratch: Iris Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> Note: You can skip over the **Introduction** section. Jump to **[k-Nearest Neighbours implementation](#k-nearest-neighbours-from-scratch)**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Introduction\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Iris Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) is perhaps the best known database to be found in the pattern recognition literature.\n",
    "\n",
    "The data set (available [here](https://www.kaggle.com/uciml/iris)) consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. **One class (specie) is linearly separable from the other 2; the latter are NOT linearly separable from each other, as seen in the plots below.**\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<a title=\"Credit: https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\"><img align=\"center\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_dataset_001.png \" style=\"height:250px;\" /></a>&nbsp;\n",
    "<a title=\"Credit: https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\"><img align=\"center\" src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_dataset_002.png \" style=\"height:250px;\" />\n",
    "\n",
    "</div>&nbsp;"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## k-Nearest Neighbours Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The [k-Nearest Neighbours](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) algorithm is a non-parametric classification method used for classification and regression. In kNN classification, an object being assigned to the class most common among its `k` nearest neighbors (k is a positive integer, typically small). If `k = 1`, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "\n",
    "![k-Nearest Neighbours](https://i.ibb.co/VQMh7p4/knn.png \"Credit: http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final1_ibdm8a.png\")&nbsp;"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> Info: Jump to **[k-Nearest Neighbours implementation](#k-nearest-neighbours-from-scratch)**.\n",
    "\n",
    "<br/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Linear Regression Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The [Linear Regression](https://en.wikipedia.org/wiki/Linear_regression) algorithm is a linear approach to modelling the relationship between a scalar response (y — dependent variables) and one or more explanatory variables (X — independent variables). Linear Regression fits a linear model with coefficients w = (w1, …, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
    "\n",
    "<a title=\"Credit: https://wiki.aalto.fi/display/ITSP/Linear+regression\"><img align=\"center\" src=\"https://wiki.aalto.fi/download/attachments/155464048/linreg.png?version=1&modificationDate=1565612055936&api=v2 \" style=\"height:300px;\" />\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> Info: Jump to **[Linear Regression implementation](#linear-regression-from-scratch)**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import random\n",
    "import csv\n",
    "import math\n",
    "import statistics\n",
    "import copy\n",
    "\n",
    "# set ramdom seed\n",
    "random.seed('iris dataset')"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# k-Nearest Neighbours From Scratch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The flowchart for implementing the kNN algorithm is shown below. Each step in the implementation will be wrapped in its own function for clarity."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<graphviz.files.Source at 0x1aa4c6138e0>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n -->\r\n<!-- Title: G Pages: 1 -->\r\n<svg width=\"933pt\" height=\"152pt\"\r\n viewBox=\"0.00 0.00 932.64 152.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 148)\">\r\n<title>G</title>\r\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-148 928.636,-148 928.636,4 -4,4\"/>\r\n<!-- 3 -->\r\n<g id=\"node1\" class=\"node\"><title>3</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\r\n<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">3</text>\r\n</g>\r\n<!-- Split Data Into Training &amp; Testing -->\r\n<g id=\"node2\" class=\"node\"><title>Split Data Into Training &amp; Testing</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"224.538\" cy=\"-18\" rx=\"134.576\" ry=\"18\"/>\r\n<text text-anchor=\"middle\" x=\"224.538\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Split Data Into Training &amp; Testing</text>\r\n</g>\r\n<!-- 3&#45;&gt;Split Data Into Training &amp; Testing -->\r\n<g id=\"edge1\" class=\"edge\"><title>3&#45;&gt;Split Data Into Training &amp; Testing</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M54.3711,-18C61.7986,-18 70.4349,-18 79.7997,-18\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"79.8024,-21.5001 89.8023,-18 79.8023,-14.5001 79.8024,-21.5001\"/>\r\n</g>\r\n<!-- Define Metrics -->\r\n<g id=\"node3\" class=\"node\"><title>Define Metrics</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"463.32\" cy=\"-18\" rx=\"63.8893\" ry=\"18\"/>\r\n<text text-anchor=\"middle\" x=\"463.32\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Define Metrics</text>\r\n</g>\r\n<!-- Split Data Into Training &amp; Testing&#45;&gt;Define Metrics -->\r\n<g id=\"edge2\" class=\"edge\"><title>Split Data Into Training &amp; Testing&#45;&gt;Define Metrics</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M359.153,-18C369.399,-18 379.49,-18 389.112,-18\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"389.355,-21.5001 399.355,-18 389.355,-14.5001 389.355,-21.5001\"/>\r\n</g>\r\n<!-- Evaluate Algorithm -->\r\n<g id=\"node4\" class=\"node\"><title>Evaluate Algorithm</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"655.306\" cy=\"-18\" rx=\"79.0865\" ry=\"18\"/>\r\n<text text-anchor=\"middle\" x=\"655.306\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Evaluate Algorithm</text>\r\n</g>\r\n<!-- Define Metrics&#45;&gt;Evaluate Algorithm -->\r\n<g id=\"edge3\" class=\"edge\"><title>Define Metrics&#45;&gt;Evaluate Algorithm</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M527.055,-18C539.336,-18 552.433,-18 565.38,-18\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"565.712,-21.5001 575.712,-18 565.712,-14.5001 565.712,-21.5001\"/>\r\n</g>\r\n<!-- 2 -->\r\n<g id=\"node5\" class=\"node\"><title>2</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-72\" rx=\"27\" ry=\"18\"/>\r\n<text text-anchor=\"middle\" x=\"27\" y=\"-68.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">2</text>\r\n</g>\r\n<!-- Calculate Distances -->\r\n<g id=\"node6\" class=\"node\"><title>Calculate Distances</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"224.538\" cy=\"-72\" rx=\"81.4863\" ry=\"18\"/>\r\n<text text-anchor=\"middle\" x=\"224.538\" y=\"-68.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Calculate Distances</text>\r\n</g>\r\n<!-- 2&#45;&gt;Calculate Distances -->\r\n<g id=\"edge4\" class=\"edge\"><title>2&#45;&gt;Calculate Distances</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M54.3711,-72C74.793,-72 104.353,-72 133.069,-72\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"133.174,-75.5001 143.174,-72 133.174,-68.5001 133.174,-75.5001\"/>\r\n</g>\r\n<!-- Get Neighbours -->\r\n<g id=\"node7\" class=\"node\"><title>Get Neighbours</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"463.32\" cy=\"-72\" rx=\"68.4888\" ry=\"18\"/>\r\n<text text-anchor=\"middle\" x=\"463.32\" y=\"-68.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Get Neighbours</text>\r\n</g>\r\n<!-- Calculate Distances&#45;&gt;Get Neighbours -->\r\n<g id=\"edge5\" class=\"edge\"><title>Calculate Distances&#45;&gt;Get Neighbours</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M306.032,-72C331.484,-72 359.549,-72 384.789,-72\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"385.047,-75.5001 395.047,-72 385.047,-68.5001 385.047,-75.5001\"/>\r\n</g>\r\n<!-- Predict Classification -->\r\n<g id=\"node8\" class=\"node\"><title>Predict Classification</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"655.306\" cy=\"-72\" rx=\"87.9851\" ry=\"18\"/>\r\n<text text-anchor=\"middle\" x=\"655.306\" y=\"-68.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Predict Classification</text>\r\n</g>\r\n<!-- Get Neighbours&#45;&gt;Predict Classification -->\r\n<g id=\"edge6\" class=\"edge\"><title>Get Neighbours&#45;&gt;Predict Classification</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M531.569,-72C539.932,-72 548.609,-72 557.309,-72\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"557.514,-75.5001 567.514,-72 557.514,-68.5001 557.514,-75.5001\"/>\r\n</g>\r\n<!-- Create Algorithm -->\r\n<g id=\"node9\" class=\"node\"><title>Create Algorithm</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"851.842\" cy=\"-72\" rx=\"72.5877\" ry=\"18\"/>\r\n<text text-anchor=\"middle\" x=\"851.842\" y=\"-68.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Create Algorithm</text>\r\n</g>\r\n<!-- Predict Classification&#45;&gt;Create Algorithm -->\r\n<g id=\"edge7\" class=\"edge\"><title>Predict Classification&#45;&gt;Create Algorithm</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M743.049,-72C751.549,-72 760.157,-72 768.61,-72\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"768.826,-75.5001 778.826,-72 768.826,-68.5001 768.826,-75.5001\"/>\r\n</g>\r\n<!-- 1 -->\r\n<g id=\"node10\" class=\"node\"><title>1</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-126\" rx=\"27\" ry=\"18\"/>\r\n<text text-anchor=\"middle\" x=\"27\" y=\"-122.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1</text>\r\n</g>\r\n<!-- Read Data -->\r\n<g id=\"node11\" class=\"node\"><title>Read Data</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"224.538\" cy=\"-126\" rx=\"48.9926\" ry=\"18\"/>\r\n<text text-anchor=\"middle\" x=\"224.538\" y=\"-122.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Read Data</text>\r\n</g>\r\n<!-- 1&#45;&gt;Read Data -->\r\n<g id=\"edge8\" class=\"edge\"><title>1&#45;&gt;Read Data</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M54.3711,-126C82.6713,-126 128.52,-126 165.352,-126\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"165.479,-129.5 175.479,-126 165.479,-122.5 165.479,-129.5\"/>\r\n</g>\r\n<!-- Clean Data -->\r\n<g id=\"node12\" class=\"node\"><title>Clean Data</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"463.32\" cy=\"-126\" rx=\"50.0912\" ry=\"18\"/>\r\n<text text-anchor=\"middle\" x=\"463.32\" y=\"-122.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Clean Data</text>\r\n</g>\r\n<!-- Read Data&#45;&gt;Clean Data -->\r\n<g id=\"edge9\" class=\"edge\"><title>Read Data&#45;&gt;Clean Data</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M273.454,-126C310.872,-126 363.057,-126 403.093,-126\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"403.254,-129.5 413.254,-126 403.254,-122.5 403.254,-129.5\"/>\r\n</g>\r\n<!-- Normalize Data -->\r\n<g id=\"node13\" class=\"node\"><title>Normalize Data</title>\r\n<ellipse fill=\"none\" stroke=\"black\" cx=\"655.306\" cy=\"-126\" rx=\"66.0889\" ry=\"18\"/>\r\n<text text-anchor=\"middle\" x=\"655.306\" y=\"-122.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Normalize Data</text>\r\n</g>\r\n<!-- Clean Data&#45;&gt;Normalize Data -->\r\n<g id=\"edge10\" class=\"edge\"><title>Clean Data&#45;&gt;Normalize Data</title>\r\n<path fill=\"none\" stroke=\"black\" d=\"M513.741,-126C533.483,-126 556.666,-126 578.45,-126\"/>\r\n<polygon fill=\"black\" stroke=\"black\" points=\"578.674,-129.5 588.674,-126 578.674,-122.5 578.674,-129.5\"/>\r\n</g>\r\n</g>\r\n</svg>\r\n"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "#hide_input\n",
    "from fastbook import gv\n",
    "\n",
    "gv(\n",
    "'''\n",
    "3->\"Split Data Into Training & Testing\"->\"Define Metrics\"->\"Evaluate Algorithm\"\n",
    "2->\"Calculate Distances\"->\"Get Neighbours\"->\"Predict Classification\"->\"Create Algorithm\"\n",
    "1->\"Read Data\"->\"Clean Data\"->\"Normalize Data\"\n",
    "'''\n",
    ")"
   ]
  },
  {
   "source": [
    "### The dataset is contained in .csv file. We will implement a main function `DataLoader` that calls several child functions. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Read Data\n",
    "\n",
    "We start with the `_load_csv()` function which will use the python [csv](https://docs.python.org/3/library/csv.html) module to read the file contents."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['5.1', '3.5', '1.4', '0.2', 'Iris-setosa'],\n",
       " ['4.9', '3.0', '1.4', '0.2', 'Iris-setosa'],\n",
       " ['6.4', '3.2', '4.5', '1.5', 'Iris-versicolor'],\n",
       " ['6.9', '3.1', '4.9', '1.5', 'Iris-versicolor'],\n",
       " ['6.2', '3.4', '5.4', '2.3', 'Iris-virginica'],\n",
       " ['5.9', '3.0', '5.1', '1.8', 'Iris-virginica']]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "#collapse-output\n",
    "def _load_csv(filename):\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = csv.reader(file)\n",
    "\t\treturn [row for row in csv_reader if row]\n",
    "\n",
    "dataset = _load_csv(\"ipynb_data/IrisData.csv\")\n",
    "dataset[:2]+dataset[51:53]+dataset[-2:]"
   ]
  },
  {
   "source": [
    "## Clean Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We can see that the csv reader loads the entire dataset as a list of list, each inner list containing `['sepal length', 'sepal width', 'petal length', 'petal width', 'species']`. We need to convert the four features from strings to floats. Let's create a function called `_clean_features()`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[5.1, 3.5, 1.4, 0.2, 'Iris-setosa'],\n",
       " [4.9, 3.0, 1.4, 0.2, 'Iris-setosa'],\n",
       " [6.4, 3.2, 4.5, 1.5, 'Iris-versicolor'],\n",
       " [6.9, 3.1, 4.9, 1.5, 'Iris-versicolor'],\n",
       " [6.2, 3.4, 5.4, 2.3, 'Iris-virginica'],\n",
       " [5.9, 3.0, 5.1, 1.8, 'Iris-virginica']]"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "#collapse-output\n",
    "def _clean_features(dataset):\n",
    "    num_columns = len(dataset[0])\n",
    "\n",
    "    for row in dataset:\n",
    "        for column in range(num_columns-1):\n",
    "            row[column] = float(row[column].strip())\n",
    "\n",
    "_clean_features(dataset)\n",
    "dataset[:2]+dataset[51:53]+dataset[-2:]"
   ]
  },
  {
   "source": [
    "Furthermore, in machine learning, it is preferred that all data be numeric (floats or integers). Thus, we need convert each unique class values (species) to integer and create a map between the integer values and the actual string values. We'll create a function called `_map_classes()` to acheieve this. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[5.1, 3.5, 1.4, 0.2, 0],\n",
       " [4.9, 3.0, 1.4, 0.2, 0],\n",
       " [6.4, 3.2, 4.5, 1.5, 1],\n",
       " [6.9, 3.1, 4.9, 1.5, 1],\n",
       " [6.2, 3.4, 5.4, 2.3, 2],\n",
       " [5.9, 3.0, 5.1, 1.8, 2]]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "#collapse-output\n",
    "def _map_classes(dataset):\n",
    "    class_mappings = {}\n",
    "    for row in dataset:\n",
    "        _specie = row[-1]\n",
    "        if _specie not in class_mappings.keys():\n",
    "            class_mappings[_specie] = len(class_mappings)\n",
    "        row[-1] = class_mappings[_specie]\n",
    "\n",
    "    return class_mappings\n",
    "\n",
    "class_mappings = _map_classes(dataset)\n",
    "print(class_mappings)\n",
    "dataset[:2]+dataset[51:53]+dataset[-2:]"
   ]
  },
  {
   "source": [
    "> Note: Now we have successfully cleaned out data and mapped the classes.\n",
    "\n",
    "<br/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Normalize Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In Machine Learning, two data scaling methods are most commonly discussed: [Normalization](https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization)) and [Standardization](https://en.wikipedia.org/wiki/Feature_scaling#Standardization_(Z-score_Normalization)).\n",
    "\n",
    "Normalization typically means rescales the values into a range of [0,1]. Standardization typically means rescales data to have a mean of 0 and a standard deviation of 1 (unit variance). This [article](https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf) analyzes the empirical results of applying different scaling methods on features in multiple experiments settings.\n",
    "\n",
    "Distance-based algorithms (such as kNN) are affected by the scale of the variables and will give higher weightage to variables which have higher magnitude. To prevent the algorithm from being biased towards variables with higher magnitude, we can bring down all the variables to the same scale.\n",
    "\n",
    "In our case, we will make use of Normalization (also known as min-max scaling) as it limits the range of the data and thus, a better option for kNN. Let's create a function called `_normalize_data()` The Normalization formula is shown below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<a title=\"Credit: https://www.kdnuggets.com/2020/04/data-transformation-standardization-normalization.html\"><img align=\"center\" src=\"https://miro.medium.com/max/1400/1*GwcC80mDUyscDAWtwZSBdA.png \" style=\"height:150px;\" />"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[0.22222222222222213,\n",
       "  0.6249999999999999,\n",
       "  0.06779661016949151,\n",
       "  0.04166666666666667,\n",
       "  0],\n",
       " [0.1666666666666668,\n",
       "  0.41666666666666663,\n",
       "  0.06779661016949151,\n",
       "  0.04166666666666667,\n",
       "  0],\n",
       " [0.5833333333333334, 0.5, 0.5932203389830508, 0.5833333333333334, 1],\n",
       " [0.7222222222222222,\n",
       "  0.4583333333333333,\n",
       "  0.6610169491525424,\n",
       "  0.5833333333333334,\n",
       "  1],\n",
       " [0.5277777777777778,\n",
       "  0.5833333333333333,\n",
       "  0.7457627118644068,\n",
       "  0.9166666666666666,\n",
       "  2],\n",
       " [0.44444444444444453,\n",
       "  0.41666666666666663,\n",
       "  0.6949152542372881,\n",
       "  0.7083333333333334,\n",
       "  2]]"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "#collapse-output\n",
    "def _normalize_data(dataset):\n",
    "    num_features = len(dataset[0])-1\n",
    "    for i in range(num_features):\n",
    "        column_values = [row[i] for row in dataset]\n",
    "        column_min = min(column_values)\n",
    "        column_max = max(column_values)\n",
    "        \n",
    "        for row in dataset:\n",
    "            row[i] = (row[i] - column_min) / (column_max - column_min)\n",
    "\n",
    "_normalize_data(dataset)\n",
    "dataset[:2]+dataset[51:53]+dataset[-2:]"
   ]
  },
  {
   "source": [
    "> Important: Now putting all the child functions together in the main function:\n",
    "\n",
    "<br/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataLoader(filename):\n",
    "    dataset = _load_csv(filename)\n",
    "    _clean_features(dataset)\n",
    "    class_mappings = _map_classes(dataset)\n",
    "    _normalize_data(dataset)\n",
    "\n",
    "    return dataset, class_mappings"
   ]
  },
  {
   "source": [
    "### Now, we're done preparing the dataset for the classification using the kNN algorithm. Next, we implement the algorithm itself in a main function `kNN_Algorithm` that calls several child functions.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Calculate Euclidean Distances "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We need to create a function that calculates the distance between two sets of data features. THere are different distance formulas availabe but the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) is most commonly used for kNN classification problems.  \n",
    "\n",
    "In mathematics, the Euclidean distance between two points in Euclidean space is the length of a line segment between the two points. The formula is shown below and is implemented with the `_euclidean_distance()` function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![Euclidean Distance Formula](https://aws1.discourse-cdn.com/codecademy/original/5X/e/2/9/6/e296139537a35d53394033117a05f12a401d4c42.png \"Credit: https://discuss.codecademy.com/t/faq-distance-formula-euclidean-distance/374303\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.21561353744805575, 0.8458718030210156, 0.9646282869629299)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "#collapse-output\n",
    "def _euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    num_features = len(row1)-1\n",
    "\n",
    "    for i in range(num_features):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    return math.sqrt(distance)\n",
    "\n",
    "(\n",
    "    _euclidean_distance(dataset[0], dataset[1]), # class 0 VS class 0\n",
    "    _euclidean_distance(dataset[0], dataset[51]), # class 0 VS class 1\n",
    "    _euclidean_distance(dataset[0], dataset[-1]) # class 0 VS class 2\n",
    ")"
   ]
  },
  {
   "source": [
    "## Get k Nearest Neighbours"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Next, we need to get the k nearest neighbours of a given (test) row (set of features) amongst a larger sample of (training) rows (sets of features). What we do is simply calculate the distances between the test row and the all the training rows and get the `k` training rows with the smallest Euclidean distances. Let's create a function `get_k_neighbours()`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[0.22222222222222213,\n",
       "  0.6249999999999999,\n",
       "  0.06779661016949151,\n",
       "  0.04166666666666667,\n",
       "  0],\n",
       " [0.1666666666666668,\n",
       "  0.41666666666666663,\n",
       "  0.06779661016949151,\n",
       "  0.04166666666666667,\n",
       "  0],\n",
       " [0.5833333333333334, 0.5, 0.5932203389830508, 0.5833333333333334, 1],\n",
       " [0.7222222222222222,\n",
       "  0.4583333333333333,\n",
       "  0.6610169491525424,\n",
       "  0.5833333333333334,\n",
       "  1],\n",
       " [0.44444444444444453,\n",
       "  0.41666666666666663,\n",
       "  0.6949152542372881,\n",
       "  0.7083333333333334,\n",
       "  2],\n",
       " [0.5277777777777778,\n",
       "  0.5833333333333333,\n",
       "  0.7457627118644068,\n",
       "  0.9166666666666666,\n",
       "  2]]"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "#collapse-output\n",
    "def _get_k_neighbours(test_row, train_data, num_neighbours):\n",
    "    test_train_distances = []\n",
    "    for train_row in train_data:\n",
    "        _test_train_distance = _euclidean_distance(test_row, train_row)\n",
    "        test_train_distances.append([train_row, _test_train_distance])\n",
    "\n",
    "    test_train_distances.sort(key=lambda idx: idx[1])\n",
    "    return [test_train_distances[i][0] for i in range(num_neighbours)]\n",
    "\n",
    "_get_k_neighbours(dataset[0], dataset[:2]+dataset[51:53]+dataset[-2:], num_neighbours=6)"
   ]
  },
  {
   "source": [
    "## Predict Classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Next, we predict the class of the test row based on the most occuring class amongst it's k nearest neighbours. We'll create a function called `_predict_classification()`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "#collapse-output\n",
    "def _predict_classification(test_row, train_data, num_neighbours):\n",
    "    nearest_neighbours =  _get_k_neighbours(test_row, train_data, num_neighbours)\n",
    "    nearest_classes = [neighbour[-1] for neighbour in nearest_neighbours]\n",
    "    predicted_class = max(set(nearest_classes), key=nearest_classes.count)\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "_predict_classification(dataset[0], dataset[:2]+dataset[51:53]+dataset[-2:], num_neighbours=6)"
   ]
  },
  {
   "source": [
    "> Important: Now putting all the child functions together in the main function:\n",
    "\n",
    "<br/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN_Algorithm(test_data, train_data, num_neighbours):\n",
    "    return [_predict_classification(test_row, train_data, num_neighbours) for test_row in test_data]"
   ]
  },
  {
   "source": [
    "### Now we can go ahead and evaluate the performance of the algorithm against the dataset. The evaluation will be  implemented using the function `Evaluate_kNN_Algorithm` that calls several child functions.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Split Dataset Into Training & Testing Samples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We will try two popular methods for splitting the dataset in training and testing samples:\n",
    "\n",
    "-  *Test/Train Split*: The dataset is shuffled and a percentage is used for training and the rest for testing. The algorithm is then trained on the training sample and evaluated it's performance evalauted with the testing sample. This is implemented in the `_test_train_split()` function.\n",
    "\n",
    "<a title=\"Credit: https://www.analyticsvidhya.com/blog/2021/05/importance-of-cross-validation-are-evaluation-metrics-enough/\"><img align=\"center\" src=\"https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/07/15185319/blogs-15-7-2020-02-1024x565.jpg \" style=\"height:300px;\" />\n",
    "\n",
    "-  *Cross Validation Split*: The dataset is split into k groups. The algorithm is then trained and evaluated k times and the performance summarized by taking the mean performance score. During each training and evaluation step, one of the k groups is used as the testing sample and the remaining groups as the training sample.  This is implemented in the `_cross_validation_split()` function.\n",
    "\n",
    "<a title=\"Credit: https://www.analyticsvidhya.com/blog/2021/05/importance-of-cross-validation-are-evaluation-metrics-enough/\"><img align=\"center\" src=\"https://zitaoshen.rbind.io/project/machine_learning/machine-learning-101-cross-vaildation/featured.png \" style=\"height:350px;\" />"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_train_split(dataset, test_ratio):\n",
    "    _dataset = copy.deepcopy(dataset)\n",
    "    random.shuffle(_dataset)\n",
    "\n",
    "    split_index = int(len(dataset) * test_ratio)\n",
    "    # Training data\n",
    "    test_sample = _dataset[0:split_index]\n",
    "    #Testing data\n",
    "    train_sample = _dataset[split_index:]\n",
    "\n",
    "    return test_sample, train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cross_validation_split(dataset, num_groups):\n",
    "    dataset_groups = []\n",
    "    _dataset = copy.deepcopy(dataset)\n",
    "    group_size = int(len(_dataset) / num_groups)\n",
    "\n",
    "    for i in range(num_groups):\n",
    "        group = []\n",
    "        while len(group) < group_size:\n",
    "            idx = random.randrange(len(_dataset))\n",
    "            group.append(_dataset.pop(idx))\n",
    "        dataset_groups.append(group)\n",
    "\n",
    "    return dataset_groups"
   ]
  },
  {
   "source": [
    "## Define Accuracy Metric "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Next, we create a simple function `_get_accuracy` that returns the percentage of the test classes correctly predicted by the algorithm."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_accuracy(test_sample, algorithm_predictions, class_mappings):\n",
    "    test_classes = [row[-1] for row in test_sample]\n",
    "    num_test_classes = len(test_classes)\n",
    "    test_labels = list(class_mappings.keys())\n",
    "\n",
    "    if len(test_classes) != len(algorithm_predictions):\n",
    "        raise IndexError(\"The count of test classes is not equal to the count of algorithm predictions!\")\n",
    "\n",
    "    num_correct_predictions = sum([actual == predicted for actual, predicted \n",
    "                                                        in zip(test_classes, algorithm_predictions)])\n",
    "\n",
    "    wrong_predictions = [f'A:{test_labels[actual]} | P:{test_labels[predicted]}'\n",
    "                                                            for actual, predicted \n",
    "                                                            in zip(test_classes, algorithm_predictions)\n",
    "                                                            if actual != predicted]\n",
    "                        \n",
    "    accuracy = (num_correct_predictions / num_test_classes) * 100\n",
    "    return accuracy, wrong_predictions"
   ]
  },
  {
   "source": [
    "## Evaluate Algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Due to the different manner in which Test/Train Split & Cross Validation Split divide the dataset, we will create two separate functions for the final algorithm evaluation:\n",
    "\n",
    "1.  `tts_Evaluate_kNN_Algorithm`: kNN algorithm evaluation using the Test/Train Split method; and\n",
    "\n",
    "2.  `cvs_Evaluate_kNN_Algorithm`: kNN algorithm evaluation using the Cross Validation Split method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tts_Evaluate_kNN_Algorithm(dataset, class_mappings, test_ratio=0.25, \n",
    "                                                                num_neighbours=3, num_iterations=100):\n",
    "    \n",
    "    ACCURACY_HISTORY = []\n",
    "    WRONG_PREDICTION_HISTORY = []\n",
    "\n",
    "    for _iter in range(num_iterations):\n",
    "        _dataset = copy.deepcopy(dataset)\n",
    "        test_sample, train_sample = _test_train_split(_dataset, test_ratio)\n",
    "\n",
    "        algorithm_predictions = kNN_Algorithm(test_sample, train_sample, num_neighbours)\n",
    "        accuracy, wrong_predictions = _get_accuracy(test_sample, algorithm_predictions, class_mappings)\n",
    "        ACCURACY_HISTORY.append(accuracy)\n",
    "        WRONG_PREDICTION_HISTORY.extend(wrong_predictions)\n",
    "\n",
    "    random.shuffle(WRONG_PREDICTION_HISTORY)\n",
    "    print('kNN algorithm evaluation using the Test/Train Split method:', '\\n\\t', \n",
    "                'Average Accuracy:', round(statistics.mean(ACCURACY_HISTORY), ndigits=4), '\\n\\t', \n",
    "                'Maximum Accuracy:', max(ACCURACY_HISTORY), '\\n')\n",
    "\n",
    "    print('A: Actual | P: Predicted')\n",
    "    print('\\n'.join(WRONG_PREDICTION_HISTORY[:20]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "kNN algorithm evaluation using the Test/Train Split method: \n\t Average Accuracy: 94.8649 \n\t Maximum Accuracy: 100.0 \n\nA: Actual | P: Predicted\nA:Iris-versicolor | P:Iris-virginica\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-versicolor | P:Iris-virginica\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-versicolor | P:Iris-virginica\nA:Iris-versicolor | P:Iris-virginica\nA:Iris-versicolor | P:Iris-virginica\nA:Iris-versicolor | P:Iris-virginica\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-virginica | P:Iris-versicolor\n"
     ]
    }
   ],
   "source": [
    "#collapse-output\n",
    "dataset, class_mappings = DataLoader(\"ipynb_data/IrisData.csv\")\n",
    "tts_Evaluate_kNN_Algorithm(dataset, class_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvs_Evaluate_kNN_Algorithm(dataset, class_mappings, num_groups=5, \n",
    "                                                                num_neighbours=3, num_iterations=100):\n",
    "    \n",
    "    ACCURACY_HISTORY = []\n",
    "    WRONG_PREDICTION_HISTORY = []\n",
    "\n",
    "    for _iter in range(num_iterations):\n",
    "        _dataset = copy.deepcopy(dataset)\n",
    "        dataset_groups = _cross_validation_split(_dataset, num_groups)\n",
    "\n",
    "        for idx, group in enumerate(dataset_groups):\n",
    "            test_sample = group\n",
    "            _train_sample = copy.deepcopy(dataset_groups)\n",
    "            del _train_sample[idx]\n",
    "            \n",
    "            train_sample = []\n",
    "            for train_group in _train_sample:\n",
    "                train_sample.extend(train_group)\n",
    "\n",
    "            algorithm_predictions = kNN_Algorithm(test_sample, train_sample, num_neighbours)\n",
    "            accuracy, wrong_predictions = _get_accuracy(test_sample, algorithm_predictions, class_mappings)\n",
    "            ACCURACY_HISTORY.append(accuracy)\n",
    "            WRONG_PREDICTION_HISTORY.extend(wrong_predictions)\n",
    "\n",
    "    random.shuffle(WRONG_PREDICTION_HISTORY)\n",
    "    print('kNN algorithm evaluation using the Cross Validation Split method:', '\\n\\t', \n",
    "                'Average Accuracy:', round(statistics.mean(ACCURACY_HISTORY), ndigits=4), '\\n\\t', \n",
    "                'Maximum Accuracy:', max(ACCURACY_HISTORY), '\\n')\n",
    "\n",
    "    print('A: Actual | P: Predicted')\n",
    "    print('\\n'.join(WRONG_PREDICTION_HISTORY[:20]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "kNN algorithm evaluation using the Cross Validation Split method: \n\t Average Accuracy: 95.22 \n\t Maximum Accuracy: 100.0 \n\nA: Actual | P: Predicted\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-versicolor | P:Iris-virginica\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-versicolor | P:Iris-virginica\nA:Iris-versicolor | P:Iris-virginica\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-versicolor | P:Iris-virginica\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-versicolor | P:Iris-virginica\nA:Iris-versicolor | P:Iris-virginica\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-virginica | P:Iris-versicolor\nA:Iris-versicolor | P:Iris-virginica\nA:Iris-versicolor | P:Iris-virginica\nA:Iris-virginica | P:Iris-versicolor\n"
     ]
    }
   ],
   "source": [
    "#collapse-output\n",
    "dataset, class_mappings = DataLoader(\"ipynb_data/IrisData.csv\")\n",
    "cvs_Evaluate_kNN_Algorithm(dataset, class_mappings)"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Discussion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We can see that the kNN algorithm evaluation using the Cross Validation Split and Test/Train Split method give similar results. A **maximum accuracy of 100%** and an **average accuracy of ~95%** across **100 iterations** using **k=3 neighbours**.\n",
    "\n",
    "A closer look at the randomly printed wrong classifications shows that the algorithm only has porblems differentiating between `Iris-virginica` and `Iris-versicolor`. It has no problem correctly classifying `Iris-setosa`.\n",
    "\n",
    "> Recall: ** Class Mappings = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2} **\n",
    "\n",
    "This confirms the statement in the Iris Dataset description that:\n",
    "\n",
    "> \"**One class (specie) is linearly separable from the other 2; the latter are NOT linearly separable from each other...**\"\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Linear Regression From Scratch\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This will be the end of this post. I will create a separate post for **Linear Regression From Scratch** and link it here.\n",
    "\n",
    "Thank you."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Resources & References"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "-  [random — Generate pseudo-random numbers](https://docs.python.org/3/library/random.html)\n",
    "\n",
    "-  [Develop k-Nearest Neighbors in Python From Scratch - Machine Learning Mastery](https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/)\n",
    "\n",
    "-  [K Nearest Neighbors Algorithm using Python From Absolute Scratch - The Nerdy Dev](https://www.youtube.com/watch?v=uclqpQe8TMQ)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> Tip: **[Jump To Top](#kNN-Linear-Regression-From-Scratch-Iris-Dataset)**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}