{
  
    
        "post0": {
            "title": "Genetic Algorithms: Weapon-Target Assignment Problem",
            "content": ". Genetic Algorithms: Weapon-Target Assignment Problem . . Note: You can skip over the Introduction section. Jump to Implementation: Weapon-Target Assignment Problem Using Python. . . Introduction . Genetic Algorithm . Genetic Algorithm (GA) is a metaheuristic inspired by Charles Darwin’s theory of natural evolution. Genetic algorithms, belonging to the larger class of Evolutionary Algorithms (EA), are commonly used to generate high-quality solutions to optimization and search problems. By relying on biologically inspired operators such as mutation, crossover, and selection, GAs reflect the process of natural selection where the fittest individuals are selected for reproduction to produce the offspring of the next generation. . The figure below provides a visualization of some common evolution terms such as gene, chromosome, and population as used in the GA domain. . . The flowchart below gives a simple overview of the steps taken in a typical GA problem from start to finish. We&#39;ll be using a slightly modified version of this in this post. . . In this post, I am not going to cover the basics of Genetic Algorithms. I&#39;m assuming that the reader has some essential knowledge of GAs and it&#39;s code implementation in Python. . If this is not the case, you can check out: . Introduction to Genetic Algorithms by Vijini Mallawaarachchi for more explanation of the steps and terms used in the flowchart above. | Simple Genetic Algorithm From Scratch in Python by Jason Brownlee for the Python code implementation of a simple GA. | . . Tip: I find Jason Brownlee&#8217;s site, Machine Learning Mastery very useful for understanding the basic code implementation of various Machine Learning algorithms and concepts. It is a good resource for getting a grasp of the basics before moving on to more advanced stuff. . Weapon-Target Assignment Problem . The Weapon-Target Assignment (WTA) problem is an optimization problem that consists of finding an optimal assignment of a set of weapons of various types to a group of targets to maximize the total expected damage done to the targets. More than one weapon can be assigned to each target, and not all targets are required to have weapons assigned. . . The basic problem is as follows (excerpt from Wikipedia): . There are a number of weapons and a number of targets. The weapons are of type i = 1,...,m. There are Wi available weapons of type i. Similarly, there are j = 1,...,n targets, each with a value of Vj. Any of the weapons can be assigned to any target. Each weapon type has a certain probability of destroying each target, given by pij. . Both static and dynamic versions of WTA can be considered. In the static case (covered in this post), the weapons are assigned to targets once. The dynamic case involves many rounds of assignment where the system&#39;s state after each exchange of fire (round) is considered in the next round. . . # Importing required modules import copy import random import time import numpy as np import matplotlib.pyplot as plt # Set random seed for reproducibility random.seed(&#39;weapon target assignment&#39;) . . . Implementation: Weapon-Target Assignment Problem Using Python . . Note: The raw notebook file can be found here. . The steps followed in this implementation are enumerated below, each within a child function. The function WEAPON_TARGET_ASSIGNMENT() wraps up the child functions, taking in the input variables and algorithm hyperparameters. . Initialize a population of firing solutions (parents): populationGenerator() | Check for infeasible solutions: checkInfeasibleSolutions() | Evaluate the performance of the firing solutions (parents): objectiveFunction() | Select the firing solutions (parents) for reproduction: RouletteWheelSelection() | Mate (crossover) the selected firing solutions (parents): Order1Crossover() | Mutate the resulting firing solutions (children): SwapBitFlipMutation() | &lt;&lt;&lt; loop &gt;&gt;&gt; | Terminate the GA evolutionary process: terminateAlgorithm() | There will also be some helper functions unrelated to GA or WTA for displaying the output firing solution in a readable format formatFiringSolution() and plotting the historical performance of firing solutions plotHistory(). . . Important: I will be working a lot with dictionaries in this WTA Python implementation. My intended approach to data manipulation is to map a key in one dictionary (representing some variable) to the same key in another dictionary (representing another variable). Of course, there are other ways (even better) of doing this. This approach (as opposed to, say, lists) makes it easier to poke under the hood and easily understand the transformations happening to the population across each implementation step. . If you don&#39;t have much experience manipulating lists and dictionaries, it is essential to note that they are both mutable objects. . A mutable object in python is assigned a unique identifier (id) on creation, with all subsequent operations on that object will keep pointing to that unique id. This article by Ventsislav Yordanov explains this in more detail. . . Note: To bring this point home, if you create a list, make a copy and modify the copy; python will also alter the original! Even though you give the original and copy different variable names, they still have the same unique id! As a sidebar, I will do precisely this: . original_list = [1, 2, 3, 4, 5] copied_list = original_list print(&quot;original&#39;s id&quot;, id(original_list)) print(&quot;copy&#39;s id&quot;, id(copied_list)) copied_list.append(6) print(&quot;after performing an operation on the copy...&quot;) print(&quot;original list&quot;, original_list) print(&quot;copied list&quot;, copied_list) . original&#39;s id 1513648680320 copy&#39;s id 1513648680320 after performing an operation on the copy... original list [1, 2, 3, 4, 5, 6] copied list [1, 2, 3, 4, 5, 6] . . As you can see, the operation on the copy also changed the original. Think of it as the operation was done on an object with a specific id, so any variables with the same id will be affected. . In a situation (such as this implementation) in which we&#39;ll create and modify copies of lists and dictionaries without altering the originals, we must make copies wholly disconnected from the sources. This is easily accomplished using python&#39;s in-built copy module, i.e.: . import copy original_list = [1,2,3,4,5] copied_list = copy.deepcopy(original_list) . . Initialize and encode the weapon and target parameters . There are three input (weapon and target) parameters in this WTA implementation. Here, it is paramount that we encode the input parameters to make them easy to access and subsequently manipulate. The input parameters and their encoding rules are enumerated below (parameter &gt;&gt; encoding): . types of weapons and their respective counts ==&gt; list of weapons; each entry is a tuple if (weapon type, count of type). | types of targets and their respective threat factors or &#39;strengths&#39; ==&gt; list of the target coefficients; target names are not important here. | weapon types and their respective probabilities of destroying each target ==&gt; list of damage probabilities; each entry is a tuple of a weapon&#39;s damage probabilities for each target. | Also, this approach makes the solution dynamic. The parameters may be modified (weapons/targets added/removed), and the code will still run fine. Thus, our input (weapon and target) parameters are given below as: . # input (weapon and target) parameters WEAPONS_INFO = [(&quot;Tank&quot;, 2), (&quot;Aircraft&quot;, 1), (&quot;Grenade&quot;, 5), (&quot;Infantry&quot;, 2)] TARGETS_FACTORS = [325, 125, 831, 10] WEAPONS_DAMAGE_PROBS = [ (0.3, 0.6, 0.7, 0.99), (0.4, 0.5, 0.75, 0.99), (0.1, 0.2, 0.2, 0.8), (0.05, 0.1, 0.01, 0.7), ] . . From the three input parameters, we will derive some child variables which will be helpful later from data manipulation, viz: . _weaponsDAMAGE_PROBS: transforming the list of weapons-damage probabilties into a dictionary with keys == weapon types and values == damage probabilties. | _nTARGETS: the total number of targets. | _totalTHREAT: the total threat posed by all targets before firing. | . We will also need a few variables for tracking the performance of the GA across generations, viz: . _histBEST_SOLUTION: keeping track of the GA&#39;s best solution, i.e., one that minimizes the total surviving target threats after firing; this will be a list of: . total surviving target threats | firing solution | best generation | . | _histMIN_SURV_THREAT: logging the minimum value (per generation) of the total surviving target threats after firing. . | _histAVG_SURV_THREAT: logging the average value (per generation) of the total surviving target threats after firing. | _countGENERATIONS: tracking the number of generations. | . Finally, we need to set the necessary hyperparameters across the implementation steps, viz: . _popSIZE: size of population per generation. | _pC: crossover hyperparameter; specifies the number of genes selected from the first parent; this value must be less than the total number of weapon instances. | _pM: mutation hyperparameter; specifies the percentage of the population to mutate. | _searchPERIOD: duration to run the algorithm; this can be a time limit or a maximum number of generations. | . # processing input (weapon and target) parameters and _weaponsDAMAGE_PROBS = { weapon[0]: list(entry) for weapon, entry in zip(WEAPONS_INFO, WEAPONS_DAMAGE_PROBS) } _nTARGETS = len(TARGETS_FACTORS) _totalTHREAT = sum(TARGETS_FACTORS) # initializing counters and logs _histBEST_SOLUTION = [0, 0, 0] _histMIN_SURV_THREAT = [] _histAVG_SURV_THREAT = [] _countGENERATIONS = 0 # setting hyperparameters _popSIZE = 100 _pC = 5 _pM = 0.45 _searchPERIOD = 5 . . Initialize a population of firing solutions (parents): populationGenerator() . We need to initialize a population of firing solutions to kickstart the evolutionary process. This initial population can be generated randomly, imported from another optimization process, or based on expert knowledge. . Here, we will randomly initialize the population. As with the input parameters, how the population is encoded can greatly ease subsequent operations, i.e., the implementation steps. . I chose to encode the firing solutions as a list of dictionaries where the each key is an instance of a weapon and its value is a one-dimensional binary encoding of the possible targets in the following format: . [{&#09; weapon_type_1: [0, 1, 0, ... , 0], &#09; weapon_type_2: [1, 0, 0, ... , 0], &#09; ... &#09; weapon_type_n: [0, 0, 0, ... , 1]}] Thus, weapon_type_1 is assigned the target at index 1, weapon_type_2 is assigned the target at index 0, and so on. . . Note: In the WTA problem, an instance of a weapon type can only be assigned to one target while multiple weapon types and instances can be assigned to a single target. . The first thing to do is create a firing solution template with no targets assigned to the weapon instances. Next, we make n (population size) copies of the template and randomly assign targets to the weapon instances. . # function for initializing random population: def populationGenerator( WEAPONS_INFO, TARGETS_FACTORS, _weaponsDAMAGE_PROBS, _nTARGETS, _popSIZE, verbose=False, ): # create an empty dictionary template for weapon types and counts with random targets assigned pop_template = {} for _type, _count in (*WEAPONS_INFO,): for idx in range(_count): pop_template[f&quot;{_type}_{idx}&quot;] = [0] * _nTARGETS if verbose: print(&quot;Firing Solution Template +--&gt; n&quot;, pop_template, &quot; n&quot;) # initialize the population with random instances of the dictionary template init_population = [] for i in range(_popSIZE): _temp = copy.deepcopy(pop_template) for value in _temp.values(): r1 = random.randint(0, _nTARGETS - 1) value[r1] = 1 init_population.append(_temp) if verbose: print( &quot;Sample Firing Solution from Initial Population +--&gt; n&quot;, random.choice(init_population), ) return init_population . . Testing the populationGenerator() with a population size of 5 parents: . test_pop = populationGenerator( WEAPONS_INFO, TARGETS_FACTORS, _weaponsDAMAGE_PROBS, _nTARGETS, _popSIZE=5, verbose=True, ) . Firing Solution Template +--&gt; {&#39;Tank_0&#39;: [0, 0, 0, 0], &#39;Tank_1&#39;: [0, 0, 0, 0], &#39;Aircraft_0&#39;: [0, 0, 0, 0], &#39;Grenade_0&#39;: [0, 0, 0, 0], &#39;Grenade_1&#39;: [0, 0, 0, 0], &#39;Grenade_2&#39;: [0, 0, 0, 0], &#39;Grenade_3&#39;: [0, 0, 0, 0], &#39;Grenade_4&#39;: [0, 0, 0, 0], &#39;Infantry_0&#39;: [0, 0, 0, 0], &#39;Infantry_1&#39;: [0, 0, 0, 0]} Sample Firing Solution from Initial Population +--&gt; {&#39;Tank_0&#39;: [1, 0, 0, 0], &#39;Tank_1&#39;: [0, 0, 1, 0], &#39;Aircraft_0&#39;: [1, 0, 0, 0], &#39;Grenade_0&#39;: [0, 1, 0, 0], &#39;Grenade_1&#39;: [0, 0, 1, 0], &#39;Grenade_2&#39;: [1, 0, 0, 0], &#39;Grenade_3&#39;: [0, 0, 0, 1], &#39;Grenade_4&#39;: [0, 0, 0, 1], &#39;Infantry_0&#39;: [0, 0, 0, 1], &#39;Infantry_1&#39;: [0, 0, 0, 1]} . . Check for infeasible solutions: checkInfeasibleSolutions() . Infeasible (firing) solutions are those that violate the rules/constraints of the problem. The relevant condition for the WTA problem is that &quot;...an instance of a weapon type can only be assigned to one target.&quot; Thus, we need to check each weapon assignment within each firing solution (parents) in the population to ensure that this rule is not broken. . How the firing solutions are encoded makes this check very straightforward. Recall that each firing solution (parent) in the population is a dictionary where the keys are weapon instances, and the corresponding values are target assignments. . Given that the target assignments are one-dimensional binary encodings, all we need to do is check the sum for each weapon instance, as follows: . If the sum of target assignments equals one, the weapon is assigned to a single target. | If the sum equals zero, the weapon is not assigned to any target. We can display a warning message in this case. | If the sum is greater than one, the weapon is assigned to multiple targets, thus violating the WTA constraint. We will set up the checkInfeasibleSolutions() function such that the algorithm is terminated if this is the case. | # function for checking the population for infeasible firing solutions: def checkInfeasibleSolutions(population, verbose=False): invalids = 0 for idx, parent in enumerate(population): for weapon_inst in parent: if sum(parent[weapon_inst]) == 0: print( f&quot;WARNING: weapon {weapon_inst}; parent {idx} not assigned to any target!&quot; ) if sum(parent[weapon_inst]) == 1: pass if sum(parent[weapon_inst]) &gt; 1: print( f&quot;ERROR: weapon {weapon_inst}; parent {idx} assigned to multiple targets!!!&quot; ) invalids += 1 if invalids == 0: if verbose: print(&quot;PASS: there are NO infeasible solutions in the population!&quot;) return False else: print(f&quot; nFAIL: there are {invalids} infeasible solutions in the population!&quot;) return True . . Testing the checkInfeasibleSolutions() function with out test population: . checkInfeasibleSolutions(test_pop, verbose=True) . PASS: there are NO infeasible solutions in the population! . False . . The checkInfeasibleSolutions() function will be added as a condition to the GA evolutionary process. If the function returns True, it will break the GA out of the evolutionary loop. . However, given how the solutions are encoded, it will be easy to avoid breaking the WTA constraints. . Evaluate the performance of the firing solutions (parents): objectiveFunction() . Evaluating the performance of the firing solutions (parents) is vital for optimizing towards the best solutions. The WTA problem is a minimization problem; the aim is to minimize the surviving target threat factors after firing. . The approach here is to isolate a firing solution (parent), &quot;fire&quot; each weapon instance at its assigned target, and subtract the damage done from the TARGETS_FACTORS. After firing all weapon instances in the parent, we get the sum of the surviving target threat factors. The best solution is the one with the lowest sum of surviving threats. . The objectiveFunction() function will return the sum of surviving threats and a list of target threat factors post-firing. The list will give a glimpse of how much damage was done to each target. . # function for calculating the surviving threat for each firing solution in the population: def objectiveFunction(population, TARGETS_FACTORS, _weaponsDAMAGE_PROBS, verbose=False): survThreatFactors = [] sum_survThreats = [] if verbose: print(f&quot;initialThreat &gt;&gt; {TARGETS_FACTORS} &gt;&gt; {sum(TARGETS_FACTORS)}&quot;) # isolate a parent in the population for idx, parent in enumerate(population): # create a copy of the target threats factors survThreat = copy.deepcopy(TARGETS_FACTORS) # go through the weapon damage probabilities for weapon_type in _weaponsDAMAGE_PROBS: # go through the weapon instances in the parent for weapon_inst in parent: # match the correct damage probability to the correct weapon instance if weapon_type in weapon_inst: # FIRE! _coeffTarget = np.multiply(parent[weapon_inst], survThreat) _probThreat = np.multiply( _weaponsDAMAGE_PROBS[weapon_type], parent[weapon_inst] ) _elimThreat = np.multiply( _weaponsDAMAGE_PROBS[weapon_type], _coeffTarget ) survThreat = np.subtract(survThreat, _elimThreat) _sumThreats = round(sum(survThreat), ndigits=2) _threatFactors = list(np.round(survThreat, decimals=2)) sum_survThreats.append(_sumThreats) survThreatFactors.append(_threatFactors) if verbose: print(f&quot;survivingThreat: parent {idx} &gt;&gt; {_threatFactors} &gt;&gt; {_sumThreats}&quot;) return sum_survThreats, survThreatFactors . . Testing the objectiveFunction() function using the test population: . sum_survThreats, survThreatFactors = objectiveFunction( test_pop, TARGETS_FACTORS, _weaponsDAMAGE_PROBS, verbose=True ) . initialThreat &gt;&gt; [325, 125, 831, 10] &gt;&gt; 1291 survivingThreat: parent 0 &gt;&gt; [236.92, 12.8, 205.67, 3.0] &gt;&gt; 458.4 survivingThreat: parent 1 &gt;&gt; [325.0, 50.0, 664.8, 0.0] &gt;&gt; 1039.8 survivingThreat: parent 2 &gt;&gt; [122.85, 100.0, 199.44, 0.04] &gt;&gt; 422.33 survivingThreat: parent 3 &gt;&gt; [292.5, 112.5, 664.8, 0.0] &gt;&gt; 1069.8 survivingThreat: parent 4 &gt;&gt; [225.08, 45.0, 49.86, 2.0] &gt;&gt; 321.94 . . Here, we can see that the fifth parent is the best and the fourth is the worst. The aim will be to select the parent 4 for reproduction so that its good genes can contribute to the next generation, while parent 3 will die out in this generation without reproducing. . At this point, we will save the average and best (minimum) performance of this generation to the _histAVG_SURV_THREAT and _histMIN_SURV_THREAT variables (lists), respectively. In addition, if this generation&#39;s best solution is better than that logged in the _histBEST_SOLUTION, we will update the _histBEST_SOLUTION log. . Select the firing solutions (parents) for reproduction: RouletteWheelSelection() . Now we need to select the parents whose genes will propagate to the next generation. The general tendency is to pick the best-performing parents. Still, it is also good to allow some bad-performing parents to reproduce, thus ensuring that the &#39;children&#39; have as much genetic diversity as possible without impacting their performance. . There are different methods in the literature for carrying out the selection process in GAs. I will use the Roulette Wheel Selection (also known as Fitness Proportionate Selection) method. I will not go into the details here, but you can check out the Wikipedia article or Roc Reguant&#39;s post for more information. . . The basic idea is as follows (excerpt from Wikipedia): . This could be imagined similar to a Roulette wheel in a casino. Usually, a proportion of the wheel is assigned to each of the possible selections based on their fitness value. This could be achieved by dividing the fitness of a selection by the total fitness of all the selections, thereby normalizing them to 1. Then a random selection is made similar to how the roulette wheel is rotated. . # function for implementing the Roulette Wheel selection method: def RouletteWheelSelection( population, popEvaluations, TARGETS_FACTORS, _weaponsDAMAGE_PROBS, verbose=False ): _popSIZE = len(population) parentsSelected = [] _invEvaluations = [1 / value for value in popEvaluations] _popProbabilities = _invEvaluations / sum(_invEvaluations) if verbose: print(&quot;parents selected for reproduction: &quot;, end=&quot;&quot;) while len(parentsSelected) &lt; _popSIZE: notChosen = True while notChosen: spin = random.random() partialSum = 0 for idx, parent in enumerate(population): partialSum += _popProbabilities[idx] if partialSum &gt; spin: parentsSelected.append(parent) notChosen = False if verbose: print(f&quot;parent {idx}&quot;, end=&quot;; &quot;) break if verbose: print() print( f&quot;initial population evaluations: {popEvaluations} &gt;&gt; average: {sum(popEvaluations)/_popSIZE:.2f}&quot; ) popEvaluations = objectiveFunction( parentsSelected, TARGETS_FACTORS, _weaponsDAMAGE_PROBS )[0] print( f&quot;selected population evaluations: {popEvaluations} &gt;&gt; average: {sum(popEvaluations)/_popSIZE:.2f}&quot; ) return parentsSelected . . Testing the RouletteWheelSelection() function using the test population: . parentsSelected = RouletteWheelSelection( test_pop, sum_survThreats, TARGETS_FACTORS, _weaponsDAMAGE_PROBS, verbose=True ) . parents selected for reproduction: parent 0; parent 0; parent 2; parent 4; parent 4; initial population evaluations: [458.4, 1039.8, 422.33, 1069.8, 321.94] &gt;&gt; average: 662.45 selected population evaluations: [458.4, 458.4, 422.33, 321.94, 321.94] &gt;&gt; average: 396.60 . . As you can see, the average performance (i.e., the sum of surviving threat factors post-firing) of the selected parents is better than that of the initial population. Note that the algorithm did not choose the bottom two worst-performing parents. . Mate (crossover) the selected firing solutions (parents): Order1Crossover() . After the parent selection, the next step is mating/reproduction, called &#39;crossover&#39; in GA terminology. I will be using the Order-1 Crossover operator because I would like to preserve the relative order genes in the parents&#39; chromosomes during the reproduction process. You can look at this article and Pablo Moscato&#39;s paper for more details. . . The basic idea is as follows (excerpt from this article): . &quot;Order 1 Crossover is a fairly simple permutation crossover. A swath of consecutive alleles from parent 1 drops down, and the remaining values are placed in the child in the order which they appear in parent 2....If you desire a second child from the two parents, flip parent 1 and parent 2...&quot; . Given that each parent is a dictionary where each key is a weapon instance and its value is a list of target assignments, we will approach the crossover step thus: . Separate the parents&#39; dictionaries keys and values into separate lists. | Apply the Order-1 Crossover operator to the parents&#39; values list to generate the children&#39;s values. | Recombine the parents&#39; keys and children values lists into dictionaries. | # function for implementing the Order 1 crossover method: def Order1Crossover( parentsSelected, _pC, TARGETS_FACTORS, _weaponsDAMAGE_PROBS, verbose=False ): _popSIZE = len(parentsSelected) # get the total number of weapon instances n_weapon_inst = len(parentsSelected[0]) # get the keys of any parent dictionary; same for all parents parents_keys = list(parentsSelected[0].keys()) # get the values of each parent dictionary parents_values = copy.deepcopy([list(parent.values()) for parent in parentsSelected]) children_values = [] while parents_values: # cut two randomly selected parents from the population try: parent1, parent2 = random.sample(parents_values, k=2) parents_values.remove(parent1) parents_values.remove(parent2) # crossover the two parents at a randomly selected point r1 = random.randint(0, n_weapon_inst - _pC - 1) # child 1 child1 = [] child1 += parent1[r1 : r1 + _pC] child1 += (parent2 * 2)[r1 + _pC : n_weapon_inst + r1] # child 1 child2 = [] child2 += parent2[r1 : r1 + _pC] child2 += (parent1 * 2)[r1 + _pC : n_weapon_inst + r1] # rearrange the order of elements in the child as required for order 1 crossover if r1 == 0: _startValue = 0 else: _startValue = n_weapon_inst - r1 child1 = (child1 * 2)[_startValue : _startValue + n_weapon_inst] child2 = (child2 * 2)[_startValue : _startValue + n_weapon_inst] # add the children to the new population children_values.append(child1) children_values.append(child2) if verbose: print( f&quot;splicing @r1={r1}: nparent1: {parent1} nX nparent2: {parent2} n==&gt; nchild1: {child1} n|| nchild2: {child2} n&quot; ) # in the case of odd-numbered populations; this will be the last crossover except ValueError: last_parent = parents_values[-1] parents_values.remove(last_parent) children_values.append(last_parent) if verbose: print( f&quot;odd-numbered population: nlast parent ==&gt; last child: n{last_parent}&quot; ) # recombine the parents&#39; keys and children values into dictionaries. childrenCrossover = [ {key: value for key, value in zip(parents_keys, child)} for child in children_values ] if verbose: print() popEvaluations = objectiveFunction( parentsSelected, TARGETS_FACTORS, _weaponsDAMAGE_PROBS )[0] print( f&quot;selected parents evaluations: {popEvaluations} &gt;&gt; average: {sum(popEvaluations)/_popSIZE:.2f}&quot; ) popEvaluations = objectiveFunction( childrenCrossover, TARGETS_FACTORS, _weaponsDAMAGE_PROBS )[0] print( f&quot;reproduced children evaluations: {popEvaluations} &gt;&gt; average: {sum(popEvaluations)/_popSIZE:.2f}&quot; ) return childrenCrossover . . Testing the Order1Crossover() function using the parentsSelected from the test population: . childrenCrossover = Order1Crossover( parentsSelected, _pC, TARGETS_FACTORS, _weaponsDAMAGE_PROBS, verbose=True ) . splicing @r1=3: parent1: [[0, 1, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]] X parent2: [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [1, 0, 0, 0], [0, 1, 0, 0]] ==&gt; child1: [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [0, 1, 0, 0]] || child2: [[0, 1, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]] splicing @r1=1: parent1: [[0, 1, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]] X parent2: [[1, 0, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1]] ==&gt; child1: [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1]] || child2: [[0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]] odd-numbered population: last parent ==&gt; last child: [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [1, 0, 0, 0], [0, 1, 0, 0]] selected parents evaluations: [458.4, 458.4, 422.33, 321.94, 321.94] &gt;&gt; average: 396.60 reproduced children evaluations: [326.2, 422.06, 444.54, 382.6, 321.94] &gt;&gt; average: 379.47 . . We can see that the average performance of the reproduced children is better than that of the selected parents. However, note that the value of the best (lowest) performance remains the same. Crossover can only get us so far as it optimizes on the current population as is. To improve the population, we need mutation. . Crossover is explorative. It discovers promising areas in the search space (i.e., gaining information on the problem) by making a big jump to a location somewhere in between two (parent) areas.WHILEMutation is exploitative. It optimizes within a promising area (i.e., using information from the parents) by creating random minor diversions, thereby staying in the parent&#39;s neighborhood. . Mutate the resulting firing solutions (children): SwapBitFlipMutation() . Now we implement the final step of the GA evolutionary process, mutation. There are different ways you can choose to introduce random variations to the population. . I intend to introduce two types of mutation to the percentage of the population set by the _pM hyperparameter, viz: . swap mutation of weapons per firing solution: swapping target assignments between weapon instances in a firing solution (child). | bit-flip mutation of targets per weapon instance: altering the target assigned to a weapon instance within a firing solution (child). | . Note: The _pM percentage will be evenly divided between the swap and bit flip methods. Furthermore, only one mutation method will be carried out on a particular firing solution in the population. . We will follow the same approach as in the crossover step i.e.: . Separate the children&#39;s dictionaries keys and values into separate lists. | Carry out mutation on the values list. | Recombine the keys and modified values lists into dictionaries. | # function for implementing mutation in the population: def SwapBitFlipMutation( childrenCrossover, _pM, TARGETS_FACTORS, _weaponsDAMAGE_PROBS, verbose=False ): _popSIZE = len(parentsSelected) # get the total number of weapon instances n_weapon_inst = len(childrenCrossover[0]) # get the keys of any child dictionary; same for all children children_keys = list(childrenCrossover[0].keys()) # get the values of each child dictionary children_values = copy.deepcopy([list(_dict.values()) for _dict in childrenCrossover]) # get the number of children that will be mutated n_to_mutate = int(_popSIZE * _pM) # randomly choose indices of the population for swap mutation idx_Swap = random.sample(range(_popSIZE - 1), k=int(n_to_mutate / 2)) # randomly choose indices of the population for bit-flip mutation idx_BitFlip = [] while len(idx_BitFlip) != len(idx_Swap): idx = random.randint(0, _popSIZE - 1) # check that the chosen index isn&#39;t selected for swap mutation if idx not in idx_Swap: idx_BitFlip.append(idx) for idx, solution in enumerate(children_values): # swap mutation if idx in idx_Swap: _solution = solution.copy() r1, r2 = random.sample(range(n_weapon_inst - 1), k=2) solution[r1] = _solution[r2] solution[r2] = _solution[r1] if verbose: print(f&quot;SWAP @child{idx} [{r1} &amp; {r2}]: n{_solution} n==&gt; n{solution} n&quot;) # bit-flip mutation if idx in idx_BitFlip: _solution = solution.copy() r1 = random.randint(0, n_weapon_inst - 1) _nTARGETS = len(solution[r1]) r2 = random.randint(0, _nTARGETS - 1) _temp = [0] * _nTARGETS _temp[r2] = 1 solution[r1] = _temp if verbose: print(f&quot;BIT-FLIP @child{idx} [{r1}]: n{_solution} n==&gt; n{solution} n&quot;) childrenMutation = [ {_key: value for _key, value in zip(children_keys, parent)} for parent in children_values ] if verbose: print() popEvaluations = objectiveFunction( childrenCrossover, TARGETS_FACTORS, _weaponsDAMAGE_PROBS )[0] print( f&quot;reproduced children evaluations: {popEvaluations} &gt;&gt; average: {sum(popEvaluations)/_popSIZE:.2f}&quot; ) popEvaluations = objectiveFunction( childrenMutation, TARGETS_FACTORS, _weaponsDAMAGE_PROBS )[0] print( f&quot;mutated children evaluations: {popEvaluations} &gt;&gt; average: {sum(popEvaluations)/_popSIZE:.2f}&quot; ) return childrenMutation . . Testing the SwapBitFlipMutation() function using the childrenCrossover from the test population: . childrenMutation = SwapBitFlipMutation( childrenCrossover, _pM, TARGETS_FACTORS, _weaponsDAMAGE_PROBS, verbose=True ) . BIT-FLIP @child1 [6]: [[0, 1, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]] ==&gt; [[0, 1, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]] SWAP @child3 [2 &amp; 8]: [[0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]] ==&gt; [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1]] reproduced children evaluations: [326.2, 422.06, 444.54, 382.6, 321.94] &gt;&gt; average: 379.47 mutated children evaluations: [326.2, 444.39, 444.54, 317.94, 321.94] &gt;&gt; average: 371.00 . . Note that the average performance of the mutated children is better than that of the reproduced children. The value of the best (lowest) performance has also improved. . Note: I ran the function a few times to get a result where the population&#8217;s best (lowest) performance improved to show that this is possible with mutation. The current population size (5) is too small to consistently get better results.However, no matter how many times you run the crossover function, the population&#8217;s best (lowest) performance will not improve; only the average will. . At this point, we&#39;re have completed one generation of the GA evolutionary process. We can check the generated children to confirm that the new population does not violate the constraints of the WTA problem. . checkInfeasibleSolutions(childrenMutation, verbose=True) . PASS: there are NO infeasible solutions in the population! . False . . Terminate the GA evolutionary process: terminateAlgorithm() . By checking for infeasible solutions amongst the new population, we have completed the first generation and started the second generation. The following steps will be evaluation, selection, crossover, and finally, mutation. This loop will continue indefinitely unless we add some stopping criteria. . If you recall, the checkInfeasibleSolutions() was implemented to terminate the algorithm if the WTA constraints are violated. Other than this, we need a way to terminate the algorithm if there are no errors in the code and no conditions are violated. Some stopping criteria include: . terminating the GA after a preset amount of time has elapsed, | terminating the GA after some generations have been reached, | terminating the GA if the performance plateaus (i.e., the best performance does not improve) across some generations. | . The terminateAlgorithm() function will implement the first two stopping criteria above. We will have the option of terminating the GA after some seconds or generations has elapsed. The firing solution with the best performance (one that minimizes the total surviving target threats after firing) up till that point will be returned as the algorithm&#39;s output. . # function for terminating the algorithm&#39;s computation; after a specified period of time (seconds): def terminateAlgorithm(count, _searchPERIOD, init=0, option=&quot;time&quot;): signal = False if option == &quot;time&quot;: running_count = count - init if running_count &gt;= _searchPERIOD: print(f&quot;{running_count:.2f} seconds limit reached, GA terminated!&quot;) signal = True if option == &quot;generations&quot;: if count &gt;= _searchPERIOD: print(f&quot;{_searchPERIOD} generations limit reached, GA terminated!&quot;) signal = True return signal . . The terminateAlgorithm() function will also be added as a condition to the GA evolutionary loop. When the preset limit is reached, the function returns True and terminates the GA. I will use the time option in this post. . . Helper functions: formatFiringSolution() &amp; plotHistory() . As discussed earlier, the formatFiringSolution() function is for displaying the output firing solution in a readable format, while the plotHistory() function is for plotting the GA&#39;s historical performance across generations. The code for both is pretty self-explanatory. You can click the buttons below to view the code. . # function for displaying the output firing solution in a readable format: def formatFiringSolution(bestFiringSolution, TARGETS_FACTORS, _weaponsDAMAGE_PROBS): # create a formatted target assignment num = 0 _list = [] targetAssigment = &quot;&quot; for key in bestFiringSolution: idx = bestFiringSolution[key].index(1) _weapon = (f&quot;{key}&quot;).ljust(11) _target = (f&quot;[Target_{idx}]&quot;).ljust(15) _list.append(f&quot;{_weapon} +--&gt; {_target}&quot;) for idx, entry in enumerate(_list): if entry[:3] != _list[idx - 1][:3]: if idx != 0: targetAssigment += &quot; n n&quot; num = 1 elif entry[:3] == _list[idx - 1][:3]: if num % 3 == 0: targetAssigment += &quot; n&quot; num += 1 targetAssigment += entry.ljust(25) # create a formatted damage assessment _damageAssessment = objectiveFunction( [bestFiringSolution], TARGETS_FACTORS, _weaponsDAMAGE_PROBS )[1][0] damageAssessment = &quot;&quot; for idx, threat in enumerate(zip(_damageAssessment, TARGETS_FACTORS)): damageAssessment += f&quot;Target_{idx}: {(1 - threat[0] / threat[1])*100:.2f}%&quot; if idx != len(_damageAssessment) - 1: damageAssessment += &quot; || &quot; return targetAssigment, damageAssessment . . . # function for plotting the GA&#39;s historical performance: def plotHistory(bestSolution, _histMIN_SURV_THREAT, _histAVG_SURV_THREAT): plt.figure(figsize=(20, 6)) plt.title( f&quot;The best performance (minimum total surviving target threats after firing) is {bestSolution}!&quot;, fontsize=&quot;xx-large&quot;, ) plt.ylabel(&quot;performance&quot;, fontsize=&quot;large&quot;) plt.xlabel(&quot;generations&quot;, fontsize=&quot;large&quot;) plt.grid(linestyle=&quot;--&quot;, which=&quot;both&quot;, axis=&quot;both&quot;) (_min,) = plt.plot(_histMIN_SURV_THREAT, label=&quot;minimum performance&quot;) (_avg,) = plt.plot(_histAVG_SURV_THREAT, label=&quot;average performance&quot;) plt.legend(handles=[_min, _avg], fontsize=&quot;x-large&quot;) plt.show() . . . Putting It All Together: WEAPON_TARGET_ASSIGNMENT() . As discussed earlier, the primary WEAPON_TARGET_ASSIGNMENT() function will wrap up all the child functions created and take in all the input variables and algorithm hyperparameters. . # the main function for solving the Weapon-Target-Assignment problem: def WEAPON_TARGET_ASSIGNMENT( WEAPONS_INFO, TARGETS_FACTORS, WEAPONS_DAMAGE_PROBS, _popSIZE, _pC, _pM, _searchPERIOD ): # processing input (weapon and target) parameters and _weaponsDAMAGE_PROBS = { weapon[0]: list(entry) for weapon, entry in zip(WEAPONS_INFO, WEAPONS_DAMAGE_PROBS) } _nTARGETS = len(TARGETS_FACTORS) _totalTHREAT = sum(TARGETS_FACTORS) # initializing counters and logs _histBEST_SOLUTION = [] _histMIN_SURV_THREAT = [] _histAVG_SURV_THREAT = [] _countGENERATIONS = 0 infeasibleSolutions = False st_time = time.time() # Initialize a population of firing solutions (parents): population = populationGenerator( WEAPONS_INFO, TARGETS_FACTORS, _weaponsDAMAGE_PROBS, _nTARGETS, _popSIZE ) # running the algorithm --&gt; while not ( terminateAlgorithm(time.time(), _searchPERIOD, st_time) or infeasibleSolutions ): # Check for infeasible solutions: infeasibleSolutions = checkInfeasibleSolutions(population) # Evaluate the performance of the firing solutions (parents): popEvaluations = objectiveFunction( population, TARGETS_FACTORS, _weaponsDAMAGE_PROBS )[0] # Log average and best (minimum) performance of this generation: _histMIN_SURV_THREAT.append(min(popEvaluations)) _histAVG_SURV_THREAT.append(sum(popEvaluations) / _popSIZE) # Update _histBEST_SOLUTION log; if better performance found idx_bestSolution = popEvaluations.index(min(popEvaluations)) if not _histBEST_SOLUTION: _histBEST_SOLUTION = [ min(popEvaluations), population[idx_bestSolution], _countGENERATIONS, ] elif min(popEvaluations) &lt; _histBEST_SOLUTION[0]: _histBEST_SOLUTION = [ min(popEvaluations), population[idx_bestSolution], _countGENERATIONS, ] # Select the firing solutions (parents) for reproduction: parentsSelected = RouletteWheelSelection( population, popEvaluations, TARGETS_FACTORS, _weaponsDAMAGE_PROBS ) # Mate (crossover) the selected firing solutions (parents): childrenCrossover = Order1Crossover( parentsSelected, _pC, TARGETS_FACTORS, _weaponsDAMAGE_PROBS ) # Mutate the resulting firing solutions (children): childrenMutation = SwapBitFlipMutation( childrenCrossover, _pM, TARGETS_FACTORS, _weaponsDAMAGE_PROBS ) # Replace the old population (parents) with the new population (children): population = childrenMutation _countGENERATIONS += 1 # Extracting and processing the best firing solution after terminating the algorithm: bestSolution = _histBEST_SOLUTION[0] bestTargetAssigment, damageAssessment = formatFiringSolution( _histBEST_SOLUTION[1], TARGETS_FACTORS, _weaponsDAMAGE_PROBS ) # Printing and logging details of the GA&#39;s solution: print( f&quot; nThe best firing solution was found in generation {_histBEST_SOLUTION[2]} / {_countGENERATIONS}!&quot; ) print(f&quot;BEST FIRING SOLUTION +--&gt; n{list(_histBEST_SOLUTION[1].values())}&quot;) print( f&quot; nThe expected total threat of the surviving targets is {bestSolution} / {_totalTHREAT}!&quot; ) print(f&quot;% DAMAGE PER TARGET +--&gt; n{damageAssessment}&quot;) print( &quot; n&quot;, &quot;&gt; TARGET ASSIGNMENTS &lt;&quot;.center(90, &quot;-&quot;), f&quot; n{bestTargetAssigment} n&quot;, &quot; n&quot; ) plotHistory(bestSolution, _histMIN_SURV_THREAT, _histAVG_SURV_THREAT) . . Finally, we run the Genetic Algorithm implementation of the Weapon-Target Assignment problem. We&#39;ll increase the number of weapon instances, target threat factors, and population size. Also, we&#39;ll let the algorithm run for five seconds. . # input (weapon and target) parameters WEAPONS_INFO = [(&quot;Tank&quot;, 5), (&quot;Aircraft&quot;, 3), (&quot;Grenade&quot;, 20), (&quot;Infantry&quot;, 5)] TARGETS_FACTORS = [729, 8054, 4547, 1120] WEAPONS_DAMAGE_PROBS = [ (0.3, 0.6, 0.7, 0.99), (0.4, 0.5, 0.75, 0.99), (0.1, 0.2, 0.2, 0.8), (0.05, 0.1, 0.01, 0.7), ] # hyperparameters _popSIZE = 100 _pC = 10 _pM = 0.5 _searchPERIOD = 5 # running the algorithm --&gt; WEAPON_TARGET_ASSIGNMENT( WEAPONS_INFO, TARGETS_FACTORS, WEAPONS_DAMAGE_PROBS, _popSIZE, _pC, _pM, _searchPERIOD ) . 5.01 seconds limit reached, GA terminated! The best firing solution was found in generation 52 / 158! BEST FIRING SOLUTION +--&gt; [[0, 1, 0, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 1, 0, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [1, 0, 0, 0], [1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 0, 1]] The expected total threat of the surviving targets is 253.45 / 14450! % DAMAGE PER TARGET +--&gt; Target_0: 80.86% || Target_1: 99.31% || Target_2: 98.80% || Target_3: 99.64% -&gt; TARGET ASSIGNMENTS &lt;- Tank_0 +--&gt; [Target_1] Tank_1 +--&gt; [Target_0] Tank_2 +--&gt; [Target_1] Tank_3 +--&gt; [Target_2] Tank_4 +--&gt; [Target_1] Aircraft_0 +--&gt; [Target_0] Aircraft_1 +--&gt; [Target_2] Aircraft_2 +--&gt; [Target_2] Grenade_0 +--&gt; [Target_1] Grenade_1 +--&gt; [Target_1] Grenade_2 +--&gt; [Target_0] Grenade_3 +--&gt; [Target_1] Grenade_4 +--&gt; [Target_1] Grenade_5 +--&gt; [Target_1] Grenade_6 +--&gt; [Target_1] Grenade_7 +--&gt; [Target_1] Grenade_8 +--&gt; [Target_2] Grenade_9 +--&gt; [Target_0] Grenade_10 +--&gt; [Target_0] Grenade_11 +--&gt; [Target_1] Grenade_12 +--&gt; [Target_3] Grenade_13 +--&gt; [Target_0] Grenade_14 +--&gt; [Target_0] Grenade_15 +--&gt; [Target_2] Grenade_16 +--&gt; [Target_1] Grenade_17 +--&gt; [Target_1] Grenade_18 +--&gt; [Target_3] Grenade_19 +--&gt; [Target_0] Infantry_0 +--&gt; [Target_0] Infantry_1 +--&gt; [Target_0] Infantry_2 +--&gt; [Target_0] Infantry_3 +--&gt; [Target_3] Infantry_4 +--&gt; [Target_3] . . . Discussion . A quick look at the printed output and the graph shows that our algorithm performed as expected—the algorithm assigned weapons to targets with the target threat factors and weapon damage probabilities taken into account. We can also see from the graph how quickly the average GA performance converged towards the optimal solution. Furthermore, GA found the best solution in the 50th generation! The high population number and mutation percentage contributed to the quick convergence. . If you recall, I mentioned in the introduction to the WTA problem that: . &quot;Both static and dynamic versions of WTA can be considered. In the static case (covered in this post), the weapons are assigned to targets once. The dynamic case involves many rounds of assignment where the system&#39;s state after each exchange of fire (round) is considered in the next round.&quot; . Thus, in this case, we pass the surviving threat factors from our static optimal solution as the new TARGETS_FACTORS variable and rerun the algorithm, specifying another stopping criterion for the dynamic steps. . . Note: The raw notebook file for this post can be found here. . This will be the end of this post. Thank you. . . Resources &amp; References . Genetic Algorithm (GA) | Introduction to Genetic Algorithms | Simple Genetic Algorithm From Scratch in Python | Weapon-Target Assignment (WTA) problem | Python Basics: Mutable vs Immutable Objects | Fitness proportionate selection | Roulette Wheel Selection in Python | Order 1 Crossover | On Genetic Crossover Operators for Relative Order Preservation | . . . Tip: Jump To Top . .",
            "url": "https://deepwilson.github.io/repository/Genetic-Algorithms-Weapon-Target-Assignment-Problem/",
            "relUrl": "/Genetic-Algorithms-Weapon-Target-Assignment-Problem/",
            "date": " • Jul 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "kNN & Linear Regression From Scratch: Iris Dataset",
            "content": "kNN &amp; Linear Regression From Scratch: Iris Dataset . . Note: You can skip over the Introduction section. Jump to k-Nearest Neighbours implementation. . . Introduction . Iris Dataset . The Iris dataset is perhaps the best known database to be found in the pattern recognition literature. . The data set (available here) consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. One class (specie) is linearly separable from the other 2; the latter are NOT linearly separable from each other, as seen in the plots below. . . k-Nearest Neighbours Algorithm . The k-Nearest Neighbours algorithm is a non-parametric classification method used for classification and regression. In kNN classification, an object is assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. . &nbsp; . . Note: Jump to k-Nearest Neighbours implementation. The raw notebook file can be found here. . Linear Regression Algorithm . The Linear Regression algorithm is a linear approach to modelling the relationship between a scalar response (y — dependent variables) and one or more explanatory variables (X — independent variables). Linear Regression fits a linear model with coefficients w = (w1, w2, … , wn) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation. . . . Note: Jump to Linear Regression implementation. . . # importing required libraries import random import csv import math import statistics import copy # set random seed random.seed(&#39;iris dataset&#39;) . . k-Nearest Neighbours From Scratch . The flowchart for implementing the kNN algorithm is shown below. Each step in the implementation will be wrapped in its own function for clarity. . . The dataset is contained in a .csv file. We will implement a function DataLoader that calls several child functions to load and cleanup the data. . Read Data . We start with the _load_csv() function which will use the python csv module to read the file contents. . def _load_csv(filename): with open(filename, &#39;r&#39;) as file: csv_reader = csv.reader(file) return [row for row in csv_reader if row] dataset = _load_csv(&quot;ipynb_data/IrisData.csv&quot;) dataset[:2]+dataset[51:53]+dataset[-2:] . [[&#39;5.1&#39;, &#39;3.5&#39;, &#39;1.4&#39;, &#39;0.2&#39;, &#39;Iris-setosa&#39;], [&#39;4.9&#39;, &#39;3.0&#39;, &#39;1.4&#39;, &#39;0.2&#39;, &#39;Iris-setosa&#39;], [&#39;6.4&#39;, &#39;3.2&#39;, &#39;4.5&#39;, &#39;1.5&#39;, &#39;Iris-versicolor&#39;], [&#39;6.9&#39;, &#39;3.1&#39;, &#39;4.9&#39;, &#39;1.5&#39;, &#39;Iris-versicolor&#39;], [&#39;6.2&#39;, &#39;3.4&#39;, &#39;5.4&#39;, &#39;2.3&#39;, &#39;Iris-virginica&#39;], [&#39;5.9&#39;, &#39;3.0&#39;, &#39;5.1&#39;, &#39;1.8&#39;, &#39;Iris-virginica&#39;]] . . . Clean Data . We can see that the csv reader loads the entire dataset as a list of list, each inner list containing [&#39;sepal length&#39;, &#39;sepal width&#39;, &#39;petal length&#39;, &#39;petal width&#39;, &#39;species&#39;]. We need to convert the four features from strings to floats. Let&#39;s create a function called _clean_features(). . def _clean_features(dataset): num_columns = len(dataset[0]) for row in dataset: for column in range(num_columns-1): row[column] = float(row[column].strip()) _clean_features(dataset) dataset[:2]+dataset[51:53]+dataset[-2:] . [[5.1, 3.5, 1.4, 0.2, &#39;Iris-setosa&#39;], [4.9, 3.0, 1.4, 0.2, &#39;Iris-setosa&#39;], [6.4, 3.2, 4.5, 1.5, &#39;Iris-versicolor&#39;], [6.9, 3.1, 4.9, 1.5, &#39;Iris-versicolor&#39;], [6.2, 3.4, 5.4, 2.3, &#39;Iris-virginica&#39;], [5.9, 3.0, 5.1, 1.8, &#39;Iris-virginica&#39;]] . . Furthermore, in machine learning, it is preferred that all data be numeric (floats or integers). Thus, we need convert each unique class values (species) to integers and create a map between the integer values and the actual string values. We&#39;ll create a function called _map_classes() to acheive this. . def _map_classes(dataset): class_mappings = {} for row in dataset: _specie = row[-1] if _specie not in class_mappings.keys(): class_mappings[_specie] = len(class_mappings) row[-1] = class_mappings[_specie] return class_mappings class_mappings = _map_classes(dataset) print(class_mappings) dataset[:2]+dataset[51:53]+dataset[-2:] . {&#39;Iris-setosa&#39;: 0, &#39;Iris-versicolor&#39;: 1, &#39;Iris-virginica&#39;: 2} . [[5.1, 3.5, 1.4, 0.2, 0], [4.9, 3.0, 1.4, 0.2, 0], [6.4, 3.2, 4.5, 1.5, 1], [6.9, 3.1, 4.9, 1.5, 1], [6.2, 3.4, 5.4, 2.3, 2], [5.9, 3.0, 5.1, 1.8, 2]] . . . Note: Now we have successfully cleaned out data and mapped the classes. . Normalize Data . In Machine Learning, two data scaling methods are most commonly discussed: Normalization&quot;) and Standardization&quot;). . Normalization typically means rescaling the data into a range of [0,1]. Standardization typically means rescaling the data to have a mean of 0 and a standard deviation of 1 (unit variance). This article analyzes the empirical results of applying different scaling methods on features in multiple experiments settings. . Distance-based algorithms (such as kNN) are affected by the scale of the variables and will give higher weightage to variables which have higher magnitude. To prevent the algorithm from being biased towards variables with higher magnitude, we can bring down all the variables to the same scale. . In our case, we will make use of Normalization (also known as min-max scaling) as it limits the range of the data and thus, a better option for kNN. Let&#39;s create a function called _normalize_data(), based on the Normalization formula shown below. . . def _normalize_data(dataset): num_features = len(dataset[0])-1 for i in range(num_features): column_values = [row[i] for row in dataset] column_min = min(column_values) column_max = max(column_values) for row in dataset: row[i] = (row[i] - column_min) / (column_max - column_min) _normalize_data(dataset) dataset[:2]+dataset[51:53]+dataset[-2:] . [[0.22222222222222213, 0.6249999999999999, 0.06779661016949151, 0.04166666666666667, 0], [0.1666666666666668, 0.41666666666666663, 0.06779661016949151, 0.04166666666666667, 0], [0.5833333333333334, 0.5, 0.5932203389830508, 0.5833333333333334, 1], [0.7222222222222222, 0.4583333333333333, 0.6610169491525424, 0.5833333333333334, 1], [0.5277777777777778, 0.5833333333333333, 0.7457627118644068, 0.9166666666666666, 2], [0.44444444444444453, 0.41666666666666663, 0.6949152542372881, 0.7083333333333334, 2]] . . . Important: Now putting all the child functions together in the main function: . def DataLoader(filename): dataset = _load_csv(filename) _clean_features(dataset) class_mappings = _map_classes(dataset) _normalize_data(dataset) return dataset, class_mappings . Now, we&#39;re done preparing the dataset for the classification using the kNN algorithm. Next, we implement the algorithm itself in a main function kNN_Algorithm that calls several child functions. . . Calculate Euclidean Distances . We need to create a function that calculates the distance between two sets of data features. There are different distance formulas available but the Euclidean distance is most commonly used for kNN classification problems. . In mathematics, the Euclidean distance between two points in Euclidean space is the length of a line segment between the two points. The formula is shown below and is implemented with the _euclidean_distance() function. . . def _euclidean_distance(row1, row2): distance = 0.0 num_features = len(row1)-1 for i in range(num_features): distance += (row1[i] - row2[i])**2 return math.sqrt(distance) ( _euclidean_distance(dataset[0], dataset[1]), # class 0 VS class 0 _euclidean_distance(dataset[0], dataset[51]), # class 0 VS class 1 _euclidean_distance(dataset[0], dataset[-1]) # class 0 VS class 2 ) . (0.21561353744805575, 0.8458718030210156, 0.9646282869629299) . . . Get k Nearest Neighbours . Next, we need to get the k nearest neighbours of a given (test) row (set of features) amongst a larger sample of (training) rows (sets of features). What we do is simply calculate the distances between the test row and all the training rows to get the k training rows with the smallest Euclidean distances. Let&#39;s create a function get_k_neighbours(). . def _get_k_neighbours(test_row, train_data, num_neighbours): test_train_distances = [] for train_row in train_data: _test_train_distance = _euclidean_distance(test_row, train_row) test_train_distances.append([train_row, _test_train_distance]) test_train_distances.sort(key=lambda idx: idx[1]) return [test_train_distances[i][0] for i in range(num_neighbours)] _get_k_neighbours(dataset[0], dataset[:2]+dataset[51:53]+dataset[-2:], num_neighbours=6) . [[0.22222222222222213, 0.6249999999999999, 0.06779661016949151, 0.04166666666666667, 0], [0.1666666666666668, 0.41666666666666663, 0.06779661016949151, 0.04166666666666667, 0], [0.5833333333333334, 0.5, 0.5932203389830508, 0.5833333333333334, 1], [0.7222222222222222, 0.4583333333333333, 0.6610169491525424, 0.5833333333333334, 1], [0.44444444444444453, 0.41666666666666663, 0.6949152542372881, 0.7083333333333334, 2], [0.5277777777777778, 0.5833333333333333, 0.7457627118644068, 0.9166666666666666, 2]] . . . Predict Classification . Next, we predict the class of the test row based on the most occuring class amongst it&#39;s k nearest neighbours. We&#39;ll create a function called _predict_classification(). . def _predict_classification(test_row, train_data, num_neighbours): nearest_neighbours = _get_k_neighbours(test_row, train_data, num_neighbours) nearest_classes = [neighbour[-1] for neighbour in nearest_neighbours] predicted_class = max(set(nearest_classes), key=nearest_classes.count) return predicted_class _predict_classification(dataset[0], dataset[:2]+dataset[51:53]+dataset[-2:], num_neighbours=6) . 0 . . . Important: Now putting all the child functions together in the main function: . def kNN_Algorithm(test_data, train_data, num_neighbours): return [_predict_classification(test_row, train_data, num_neighbours) for test_row in test_data] . Now, we can go ahead and evaluate the performance of the algorithm against the dataset. The evaluation will be implemented using the function Evaluate_kNN_Algorithm which calls several child functions to split the dataset into test/train samples and calculate accuracies. . . Split Dataset Into Training &amp; Testing Samples . We will try two popular methods for splitting the dataset in training and testing samples: . Test/Train Split: The dataset is shuffled and a percentage is used for training and the rest for testing. The algorithm is then trained on the training sample and it&#39;s performance evaluated using the testing sample. This is implemented in the _test_train_split() function. | . . def _test_train_split(dataset, test_ratio): _dataset = copy.deepcopy(dataset) random.shuffle(_dataset) split_index = int(len(dataset) * test_ratio) # Training data test_sample = _dataset[0:split_index] #Testing data train_sample = _dataset[split_index:] return test_sample, train_sample . Cross Validation Split: The dataset is shuffled and split into k groups. The algorithm is then trained and evaluated k times and the performance summarized by taking the mean performance score. During each training and evaluation step, one of the k groups is used as the testing sample and the remaining groups as the training sample. This is implemented in the _cross_validation_split() function. | . . def _cross_validation_split(dataset, num_groups): dataset_groups = [] _dataset = copy.deepcopy(dataset) group_size = int(len(_dataset) / num_groups) for i in range(num_groups): group = [] while len(group) &lt; group_size: idx = random.randrange(len(_dataset)) group.append(_dataset.pop(idx)) dataset_groups.append(group) return dataset_groups . . Define Accuracy Metric . Next, we create a simple function _get_accuracy() that returns the percentage of the test classes correctly predicted by the algorithm. . def _get_accuracy(test_sample, algorithm_predictions, class_mappings): test_classes = [row[-1] for row in test_sample] num_test_classes = len(test_classes) test_labels = list(class_mappings.keys()) if len(test_classes) != len(algorithm_predictions): raise IndexError(&quot;The count of test classes is not equal to the count of algorithm predictions!&quot;) num_correct_predictions = sum([actual == predicted for actual, predicted in zip(test_classes, algorithm_predictions)]) wrong_predictions = [f&#39;A:{test_labels[actual]} | P:{test_labels[predicted]}&#39; for actual, predicted in zip(test_classes, algorithm_predictions) if actual != predicted] accuracy = (num_correct_predictions / num_test_classes) * 100 return accuracy, wrong_predictions . Evaluate Algorithm . Due to the different manners in which Test/Train Split &amp; Cross Validation Split divide the dataset, we will create two separate functions for the final algorithm evaluation: . tts_Evaluate_kNN_Algorithm: kNN algorithm evaluation using the Test/Train Split method; and . | cvs_Evaluate_kNN_Algorithm: kNN algorithm evaluation using the Cross Validation Split method. . | def tts_Evaluate_kNN_Algorithm(dataset, class_mappings, test_ratio=0.25, num_neighbours=3, num_iterations=100): ACCURACY_HISTORY = [] WRONG_PREDICTION_HISTORY = [] for _iter in range(num_iterations): _dataset = copy.deepcopy(dataset) test_sample, train_sample = _test_train_split(_dataset, test_ratio) algorithm_predictions = kNN_Algorithm(test_sample, train_sample, num_neighbours) accuracy, wrong_predictions = _get_accuracy(test_sample, algorithm_predictions, class_mappings) ACCURACY_HISTORY.append(accuracy) WRONG_PREDICTION_HISTORY.extend(wrong_predictions) random.shuffle(WRONG_PREDICTION_HISTORY) print(&#39;kNN algorithm evaluation using the Test/Train Split method:&#39;, &#39; n t&#39;, &#39;Average Accuracy:&#39;, round(statistics.mean(ACCURACY_HISTORY), ndigits=4), &#39; n t&#39;, &#39;Maximum Accuracy:&#39;, max(ACCURACY_HISTORY), &#39; n&#39;) print(&#39;A: Actual | P: Predicted&#39;) print(&#39; n&#39;.join(WRONG_PREDICTION_HISTORY[:20])) . dataset, class_mappings = DataLoader(&quot;ipynb_data/IrisData.csv&quot;) tts_Evaluate_kNN_Algorithm(dataset, class_mappings) . kNN algorithm evaluation using the Test/Train Split method: Average Accuracy: 94.8649 Maximum Accuracy: 100.0 A: Actual | P: Predicted A:Iris-versicolor | P:Iris-virginica A:Iris-virginica | P:Iris-versicolor A:Iris-virginica | P:Iris-versicolor A:Iris-virginica | P:Iris-versicolor A:Iris-versicolor | P:Iris-virginica A:Iris-virginica | P:Iris-versicolor A:Iris-virginica | P:Iris-versicolor A:Iris-virginica | P:Iris-versicolor A:Iris-virginica | P:Iris-versicolor A:Iris-versicolor | P:Iris-virginica A:Iris-versicolor | P:Iris-virginica A:Iris-versicolor | P:Iris-virginica A:Iris-versicolor | P:Iris-virginica A:Iris-virginica | P:Iris-versicolor A:Iris-virginica | P:Iris-versicolor A:Iris-virginica | P:Iris-versicolor A:Iris-virginica | P:Iris-versicolor A:Iris-virginica | P:Iris-versicolor A:Iris-virginica | P:Iris-versicolor A:Iris-virginica | P:Iris-versicolor . . . def cvs_Evaluate_kNN_Algorithm(dataset, class_mappings, num_groups=5, num_neighbours=3, num_iterations=100): ACCURACY_HISTORY = [] WRONG_PREDICTION_HISTORY = [] for _iter in range(num_iterations): _dataset = copy.deepcopy(dataset) dataset_groups = _cross_validation_split(_dataset, num_groups) for idx, group in enumerate(dataset_groups): test_sample = group _train_sample = copy.deepcopy(dataset_groups) del _train_sample[idx] train_sample = [] for train_group in _train_sample: train_sample.extend(train_group) algorithm_predictions = kNN_Algorithm(test_sample, train_sample, num_neighbours) accuracy, wrong_predictions = _get_accuracy(test_sample, algorithm_predictions, class_mappings) ACCURACY_HISTORY.append(accuracy) WRONG_PREDICTION_HISTORY.extend(wrong_predictions) random.shuffle(WRONG_PREDICTION_HISTORY) print(&#39;kNN algorithm evaluation using the Cross Validation Split method:&#39;, &#39; n t&#39;, &#39;Average Accuracy:&#39;, round(statistics.mean(ACCURACY_HISTORY), ndigits=4), &#39; n t&#39;, &#39;Maximum Accuracy:&#39;, max(ACCURACY_HISTORY), &#39; n&#39;) print(&#39;A: Actual | P: Predicted&#39;) print(&#39; n&#39;.join(WRONG_PREDICTION_HISTORY[:20])) . dataset, class_mappings = DataLoader(&quot;ipynb_data/IrisData.csv&quot;) cvs_Evaluate_kNN_Algorithm(dataset, class_mappings) . kNN algorithm evaluation using the Cross Validation Split method: Average Accuracy: 95.22 Maximum Accuracy: 100.0 A: Actual | P: Predicted A:Iris-virginica | P:Iris-versicolor A:Iris-versicolor | P:Iris-virginica A:Iris-virginica | P:Iris-versicolor A:Iris-virginica | P:Iris-versicolor A:Iris-versicolor | P:Iris-virginica A:Iris-versicolor | P:Iris-virginica A:Iris-virginica | P:Iris-versicolor A:Iris-virginica | P:Iris-versicolor A:Iris-versicolor | P:Iris-virginica A:Iris-virginica | P:Iris-versicolor A:Iris-versicolor | P:Iris-virginica A:Iris-versicolor | P:Iris-virginica A:Iris-virginica | P:Iris-versicolor A:Iris-virginica | P:Iris-versicolor A:Iris-virginica | P:Iris-versicolor A:Iris-virginica | P:Iris-versicolor A:Iris-virginica | P:Iris-versicolor A:Iris-versicolor | P:Iris-virginica A:Iris-versicolor | P:Iris-virginica A:Iris-virginica | P:Iris-versicolor . . . Discussion . We can see that the kNN algorithm evaluation using the Cross Validation Split and Test/Train Split method give similar results: maximum accuracy of 100% and an average accuracy of ~95% across 100 iterations using k=3 neighbours. . A closer look at the randomly printed wrong classifications shows that the algorithm only has porblems differentiating between Iris-virginica and Iris-versicolor. It has no problem correctly classifying Iris-setosa. . Recall: Class Mappings = {&#39;Iris-setosa&#39;: 0, &#39;Iris-versicolor&#39;: 1, &#39;Iris-virginica&#39;: 2} . This confirms the statement in the Iris Dataset description that: . &quot;One class (specie) is linearly separable from the other 2; the latter are NOT linearly separable from each other...&quot; . . Note: The raw notebook file for this post [k-Nearest Neighbours From Scratch] can be found here. . . Linear Regression From Scratch . This will be the end of this post. I will create a separate post for Linear Regression From Scratch and link it here. . Thank you. . . Resources &amp; References . random — Generate pseudo-random numbers . | Develop k-Nearest Neighbors in Python From Scratch - Machine Learning Mastery . | K Nearest Neighbors Algorithm using Python From Absolute Scratch - The Nerdy Dev . | . . . Tip: Jump To Top . .",
            "url": "https://deepwilson.github.io/repository/kNN-Linear-Regression-Iris_Dataset/",
            "relUrl": "/kNN-Linear-Regression-Iris_Dataset/",
            "date": " • Jun 27, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Change Log",
            "content": "Editing The Look Of Flash Alerts . file: _sass minima fastpages-styles.scss . code: .flash { position: relative; padding: 10px 10px; border-style: solid; border-width: 4px; border-radius: 10px; } . . Editing The Look Of Inline Markdown Code . file: _sass minima colorschemes fastpages-dracula-highlight.scss . code: .highlight { // background: $dt-code-cell-background !important; color: $dt-gray-light !important; pre, code { background: $dt-code-cell-background; color: $dt-gray-light; border-left: 10px solid $dt-code-cell-background; border-left: 10px solid $dt-code-cell-background; border-top: 10px solid $dt-code-cell-background; border-bottom: 10px solid $dt-code-cell-background; border-radius: 15px !important; } . . Editing The Look Of Jupyter Code Cells . file: _sass minima syntax_highlight_base.scss . code: some code needed to be commented out due to style conflicts in other .scss files . // .input_area pre, .input_area div { // margin-bottom: 2rem !important; // margin-top: 1.5rem !important; // padding-bottom: 0 !important; // padding-top: 0 !important; // background: $dt-code-cell-background; // -webkit-font-smoothing: antialiased; // text-rendering: optimizeLegibility; // font-family: Menlo, Monaco, Consolas, &quot;Lucida Console&quot;, Roboto, Ubuntu, monospace; // border-radius: 5px; // font-size: 100%; // font-weight: 350; // make code have slightly more weight than text // } .input_area pre { border-left: 10px solid $dt-code-cell-background; border-left: 10px solid $dt-code-cell-background; border-top: 10px solid $dt-code-cell-background; border-bottom: 10px solid $dt-code-cell-background; border-radius: 10px !important; } . . Fix Scrollbars Not Showing For Overflowing Notebook Cells . file: _sass base_typography.scss . code: .re { -webkit-overflow-scrolling: touch; font-family: _font(family-fixed); font-size: 0.9rem; margin: 0 0 _size(element-margin) 0; // this fixed the issue with scrollbars overflow: auto !important; code { display: block; line-height: 1.75; padding: 1rem 1.5rem; } } . . Editing The Size Of Normal Text . file: _sass base_typography.scss . code: body, input, select, textarea { // controls the font type of the blog text (unformatted markdowns) font-family: _font(family); font-weight: _font(weight); font-size: 1rem; line-height: 2.375; } . . Fix #Collapse-Output Not Working For Notebooks . file: _action_files hide.tpl . code: replace the pertinent block . {% block output_group -%} {%- if cell.metadata.collapse_output -%} &lt;details class=&quot;description&quot;&gt; &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Output&quot; data-close=&quot;Show Output&quot;&gt;&lt;/summary&gt; &lt;p&gt;{{ super() }}&lt;/p&gt; &lt;/details&gt; &lt;br/&gt; &lt;br/&gt; {%- elif cell.metadata.hide_output -%} {%- else -%} {{ super() }} {%- endif -%} {% endblock output_group %} . . Editing The Look Of Images . file: _sass minima fastpages-styles.scss . code: .post img { display: block; vertical-align: top; margin-left: auto; border: groove; margin-right: auto; } . . Fix Posts Not Hiding . files: index.html; _layouts blog.html; _layouts categories.html; _layouts tags.html . add hide: true to the front matter of the post; you could also set search_exclude = true but that means the only way the hidden post can be accessed is through its permalink (for example, setting permalink: /hidden/:title/) . code: {% for post in ***** %} {% if post.hide != true %} ***** {% endif %} {% endfor %} . . . template . ### editing ... file: &lt;details&gt; &lt;summary markdown=&quot;span&quot;&gt;&lt;strong&gt;code:&lt;/strong&gt;&lt;/summary&gt; scss **code** &lt;/details&gt; &lt;br/&gt; &lt;br/&gt; .",
            "url": "https://deepwilson.github.io/repository/Change-Log/",
            "relUrl": "/Change-Log/",
            "date": " • Jun 25, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Setting Up This Blog",
            "content": "Setting Up This Blog Using fastpages . . Note: You can skip over the Background section. I went off for a while here. Jump to Introducing fastpages . . Background . Over the course of this year, as I got deeper into Machine Learning and Computer Vision, I started to come across posts and comments advising folks new to programming in general to create a blog detailing their experiences. . Most posts/people summed up the benefits of creating such a blog along the lines of: . You are presently in the best position to help along another newcomer who is a few steps behind you. This is because the issues you came across and the mistakes you made are still fresh in your mind and the resources you consulted to overcome/correct those issues/mistakes are still fresh in you mind and close to hand. | It is generally agreed that writing about a topic helps deepen your understanding of the topic and force you to address the gaps in your knowledge. | You write for totally selfish reasons. You write for posterity. You write for your future self. | . Given the schools of thought I was exposed to while learning programming and my personal way of doing things, my code is always littered with detailed and sometimes superfluous comments. Thus, I initially assumed that all I needed to do was put all my comments for a project together, edit and format them into a coherent document, and voila!, blog post. . Unfortunately, the various resources I found online for creating simple blogs were overly involved and required knowledge of html and other web-related stuff of which I knew nothing. Furthermore, I wasn&#39;t interested in going off on a tangent when my focus is Machine Learning and Computer Vision. I also came across GitHub Pages at this point but it was more of the same. . So, I gave up on creating a blog and stuck to writing detailed comments in my code. . A couple months after, I took fast.ai&#39;s Practical Deep Learning for Coders online course. At some point in Lesson 3, when Jeremy Howard started going on about writing and blogs, in my mind I went &quot;Here we go again.&quot; Then he introduced fastpages and it was all I ever needed. . Create a blog using jupyter notenooks and markdown files? Take my money!!! . . Introducing fastpages . fastpages is an easy to use blogging platform, with support for Jupyter notebooks, Word documents, and Markdown. . . fastpages uses GitHub Actions to simplify the process of creating Jekyll blog posts on GitHub Pages from a variety of input formats. . See the fastpages GitHub repository for a more detailed list of features. . Note: See the demo site. . . Initial Setup . The initial setup is totally straightforeward and effortless. The setup instructions are detailed and there is even a walkthrough on YouTube if that wasn&#39;t enough. . It wasn&#39;t enough for me. There was part of the setup where I needed to merge a pull request but I somehow didn&#39;t see that line in the instructions. . With the setup instructions and YouTube walkthrough, you can&#39;t go wrong. If you, go through either of them again, slowly. You most definitely missed something. . If at any point you come across build issues, refer to the fastpages Troubleshooting Guide. . . Customizing Your Blog . Once you&#39;re done with the intial setup, you&#39;ll get a generic blog that contains a few placeholders from fastbook. There are a few simple customizations you can make to put your personal stamp on your blog. . These customizations can be implemented across four files and one folder in the newly forked blog repository. I will list the files and the personalizations that can be done in each. . index.html . This file is responsible for the the content of the blog&#39;s homepage -- between the top bar and the Posts section. The picture below shows the code and the result in the homepage. . . Two sets of changes can be made in this file: . Line 4: The blog logo by sepecified by a file placed in the images folder of the base directory. This is the image preview automatically shown with your blog URL on social media sites. You can either: . replace the fastpages logo with one of your own choosing, leaving Line 4 as it is; or | add you own file to the images folder and edit the filename in Line 4. . . Important: The image for the blog logo MUST be in the images folder (or subfolders) for your blog to render correctly. . | . | Lines 6-14: Everything here can be removed and replaced with content of your choosing. From my experience, the content can be in html format or markdown format or a mix of both formats. | README.md . This file is responsible for the content of the blog&#39;s github repository page -- after the folders and files have been listed. . . Two sets of changes can be made to this file: . Line 6: You can totally remove that line, thus removing the View Demo Site badge. On the other hand, you might want to change it to one of your choosing and make it point to another link. . This is quite simple. The shields.io developer site contains example code on how to create your own custom badges. If you want to use a shield logo other than the ones available, you need to convert your .png to base64 code, you can&#39;t just specify a path to the .png file. b64.io is a free resource for converting images to base64 format. The logo is then specified in the badge code using the snippet below. . ?logo=data:image/png;base64,… . You can consult the answers provided to this stackoverflow question if you have any difficulty creating you custom badge. You can create any number of badges this way, all with different attributes. . | Lines 7-554: Everything here can be removed and replaced with content of your choosing. The content has to be in markdown format. . | _config.yml . This is the main configuration file for the blog. This file should be edited carefully as any mistakes will cause errors when GutHub tries to build your blog. The main fastpages README.md provides detailed information on what parts of the blog the parameters in this file control. . However, since we&#39;re talking about simple customizations, here are some: . Line 9: This controls the title of the blog found at the top of the homepage. It is freely customizable. | Line 10: This controls the blog description at the very bottom of the homepage. It is freely customizable. | Lines 39-43: Here you can add links you your social media accounts and other professional accounts. This link provides the list if supported social networks and their keys. | Line 52: This controls the display of image previews on the home page, for posts that have them. It is set to false by default. You should try it out, it&#39;s a good look. | images . This folder contains images necessary for customizing the look of the blog&#39;s URL. There are a couple of changes you can make here. . logo.png: As stated earlier, this specfies the image preview automatically shown with your URL on social media sites. This is freely customizable. | favicon.ico: This controls the page icon shown on the browser tab when the site is opened in browser. There are lots of free resources for downloading .ico files or converting your .png to .ico. | . . Important: From my experience, the file name of the page icon MUST be favicon.ico. . pages/about.md . This file is responsible for the the content of the blog&#39;s About Me page -- found on right-side of the top bar. The picture below shows the code and the result on the blog. . . Everything from Lines 7-12 can be removed and replaced with content of your choosing. The content has to be in markdown format. . . Note: That&#8217;s about it for simple customizations&#8217; you can make your blog at first go. . . Further Customizations . Here are some other advanced customizations you can carry out. They are advanced in the sense that you need to go through the documentation of the various resources used by fastpages in order to figure out to to modify them. . At this point, you&#39;ll be diverging greatly from the default fastpages setup for your blog, thus you&#39;ll need to modify/add multiple files to successfully make a modification. . Switching To Dark Mode: . Prudhvi Rampey provides a simple guide to being the darkmode experience to fastpages. All you have to do is add one file (available here) to the /_sass/minima/ folder and add this import line in custom-styles.scss in the same folder. . /*--*/ /*-- ADD YOUR STYLES BELOW -*/ @import &quot;minima/dark-mode&quot;; . Prashanth Rao&#39;s article which talks generally about the advantages of using fastpages also contains a few tips on visual customizations in the appendix section. . Making Your Blog Appear On Google Search . Victor2Code has a post on how to make your site and pages appear on Google search result. It only takes two steps. . NOTE:Victor notes in his post that amongst the ownership verification options provided by Google, he was unable to verify using the fist option didn&#39;t woer for him.&gt; However, it worked for me in flawlessly. So, I will advise that you still try that option first (as it is by far the easiest). . Adding A Copy Button To Markdown Code Blocks . Steve provides a guide on how to achieve this. Aleksandr Hovhannisyan also has a guide which follows a slightly different implementation to acheive the same goal. . Adding Flash Alerts To Your Posts . . Flash alerts like this can be used to highlight some information. A comprehensive guide on styling flash alerts can be found here. . Dynamically Run Code Written On Jupyter Notebooks . If you have a post that contains charts or tables with data that changes over time, the default fastpages setup means you will have to manually re-run the source notebook and rebuild the site. Niegil Francis provides a guide on setting up your blog to automatically runs and updates itself so that your page is always up-to-date with the current data. . Creating Permalinks . This Jekyll documentation provides information on how to set up permalinks for you blog. However, I found that this post did a better job of explaining how Jekyll permalinks work and how to customize them to fit your blog. This is also useful. . Creating Distinctive README.md and about.md . The README.md file in your blog repository&#39;s root directory and the about.md file in the _pages folder are markdown files you can use to provide some information about your blog and yourself. The syntax for both files are the same (since they are both markdown files). . Now you can get really inventive with the styling and content of these files, as some folks have. Matias Singers has created a &#39;curated list of awesome READMEs&#39;. Abhishek Naidu went some steps further by creating a more comprehensive &#39;curated list of awesome Github Profile READMEs&#39; broken down into categories and adding references to tools, articles and YouTube tutorials. . No use reinventing the wheel. Go through both lists and look at the README&#39;s. When you see a style that appeals to you, check out the raw file and steal shamelessly from the best! . You can find markdown badges here and github readme stats here. . . Miscellaneous Issues . These are a bunch of random, mostly minor issues I came across and the resources I used to solve them: . BUILD ISSUES: I had a bunch of build issues while creating this blog post, all because of malformed front matter. It turns out the jekyll is quite unforgiving of syntax errors with respect to the front matter. Have a look at fastpages&#39;s Frequent Errors and Configure Title &amp; Summary to avoid the same pitfalls. | . INCLUDING YOUTUBE LINKS IN MARKDOWN FILES: Jupyter notebooks have an easy way to display tweetcards:&gt; twitter: https://twitter.com/Twitter/status/1398341197047939073?s=20 and YouTube videos: youtube: https://www.youtube.com/watch?v=vJiZqZRkIg8. Markdown files have a similar way to display tweetcards: _{_%_ https://twitter.com/Twitter/status/1398341197047939073?s=20 _%_}_ around the link to the tweet (Note: I added underscores because the syntax detection is quite aggressive.). However, there isn&#39;t a similar method for YouTube videos. . I poked around on stackoverflow and the answers to this question provide multiple solutions for displaying YouTube videos. . | . GITIGNORING .IPYNB_CHECKPOINTS: In order to ignore those pesky .ipynb_checkpoints/ folders everywhere in your blog repository, add the following lines to your .gitignore file: . .ipynb_checkpoints */.ipynb_checkpoints/* . and you should be good to go. Courtesy of stackoverflow. . Alternatively, you can use the python code below to delete all auto-generated temporary folders. You can place the code in a python file and run that file before pushing your code to GitHub: . import os import shutil local_repository_path = r&quot;local/repository/path&quot; def clean_blog_repo(targetFolder=local_repository_path): deleted_folders = [] for dirpath, dirnames, _ in os.walk(targetFolder): for folder in dirnames: if folder in [&quot;__pycache__&quot;, &quot;.ipynb_checkpoints&quot;]: # you can add more folders folderpath = os.path.join(dirpath, folder) deleted_folders.append(f&quot;{folder}: {folderpath}&quot;) shutil.rmtree(folderpath) if len(deleted_folders) != 0: print(&quot;The following folders were deleted: n&quot;, &quot; n&quot;.join(deleted_folders)) else: print(&quot;No folders were found!&quot;) clean_blog_repo() . | . SHOW HIDDEN FILES IN JUPYTER LAB: If you make use of JupyterLab, its file explorer hides hidden files by default and there isn&#39;t an ooption in the setting to control that behaviour. To display hidden files, courtesy of GitHub: While running the jupyter kernel in the terminal, do NOT use jupyter lab, use jupyter lab --ContentsManager.allow_hidden=True . | . FIX AUTOCOMPLETE IN JUPYTERLAB: I also had an issue where the autocomplete in JupyterLab wasn&#39;t working. It turns out that the issue is due to an outdated jedi dependency. This is easily fixed by running pip install jedi==0.17.2 in the terminal. Once again, courtesy of stackoverflow. | . MARKDOWN CHEATSHEET: A series of comprehensive Markdown cheatsheats have been compiled by some awesome folks out there. You can check them out for reference: . fefong on GitHub. | adam-p also on GitHub | Matt Cone; this being my personal best. | . | . RENDERING HTML IMAGES FROM JUPYTER NOTEBOOK: I had issues with images locally refernced images showing on the blog. I would link to an image using the usual syntax: ![](../path/to/image.png) and the image will show up as expected in jupyter notebook. However, on the blog, nothing! . I went through this thread on the fastai forum and its not just me. The best way to ensure that your images (or other data) render as expected on the blog is to remotely reference the image on GitHub. Thus . Local Reference: ![](../path/to/image.png) . Remote Reference: ![](https://raw.githubusercontent.com/github-profile/blog-repo/default-branch/path/to/image.png) . NOTE:In my case, my default branch is main, yours may be master. . | . Conclusion . This is all I have for now. Hope you found the post useful. I plan to keep updating this post until I&#39;m done tweaking the looks of this blog. . I just want to add that I created two draft versions of this post:one in jupyter notebook and the other in markdown. I found the notebook version more comfortable to work with and the final output better looking than the markdown version. . . . Tip: Jump To Top . .",
            "url": "https://deepwilson.github.io/repository/Setting-Up-This-Blog/",
            "relUrl": "/Setting-Up-This-Blog/",
            "date": " • Jun 14, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Draft_ Setting Up This Blog Using fastpages",
            "content": ". Setting Up This Blog Using fastpages . . Background . . You can skip over this **Background** section. I went off for a while here. Over the course of this year when I got deeper into Machine Learning and Computer Vision, I started to come across posts and comments advising folks new to programming in general to create a blog detailing their journey. The benefits of creating such a blog could be summed up as: . You are presently in the best position to help along another newcomer who is a few steps behind you. This is because the issues you came across and the mistakes you made are still fresh in your mind and the resources you consulted to overcome/correct those issues/mistakes are still fresh in you mind and close to hand. | It is generally agreed that writing about a topic helps deepen your understanding of the topic and force you to address the gaps in your knowledge. | You write for totally selfish reasons. You write for posterity. You write for your future self. | . Given the schools of thought I was exposed to while learning programming and my personal way of doing things, my code is always littered with detailed and sometimes superfluous comments. Thus, initially it seemed all I needed to do was put all my comments for a project together, edit and format into a coherent document and voila! blog post. . Unfortunately, the various resources I found online for creating simple blogs were overly involved and required knowledge of html and other web-related stuff of which I knew nothing. Furthermore, I wasn’t interested in going on a tangent when my focus id Machine Learning and Computer Vision. I come across GitHub Pages at this point but it was more of the same. . So, I gave up on creating a blog and stuck to writing detailed comments in my code. . Then I took fast.ai’s Practical Deep Learning for Coders online course. At some point in Lesson 3, when Jeremy Howard started going on about writing and blogs, I went all “Here we go again.” Then he introduced {fastpages](https://github.com/fastai/fastpages) and it was all I ever needed. . Create a blog using jupyter notenooks and markdown files? Take my money!!! . . Introducing fastpages . fastpages is an easy to use blogging platform, with support for Jupyter notebooks, Word docs, and Markdown. . . fastpages uses GitHub Actions to simplify the process of creating Jekyll blog posts on GitHub Pages from a variety of input formats. . See the fastpages GitHub repository for a more detailed list of features. . . **[See the demo site.](https://fastpages.fast.ai/)** . Initial Setup . The initial setup is totally straightforeward and effortless. The setup instructions are detailed and there is even a walkthrough on YouTube if that wasn’t enough. . It wasn’t enough for me. There was part of the setup where I needed to merge a pull request but I somehow didn’t see that line in the instructions. . With the setup instructions and YouTube walkthrough, you can’t go wrong. If you, go through either of them again, slowly. You most definitely missed something. . Customizing Your Blog . Once you’re done with the intial setup, you’ll get a generic blog that contains a few placeholders from fastbook. There are a few simple changes you can make to put your personal stamp on your blog. . These changes can be implemented across three files in you new blog repository. I will list the files and the personalizations that can be done in each. . index.html . This file is responsible for the the content of the blog’s homepage – between the top bar and the Posts section. The picture below shows the code and the result in the homepage. . . Two sets of changes can be made in this file: . Line 4: The blog logo by sepecified by a file placed in the images folder of the base directory. This is the image preview automatically shown with your blog URL on social media sites. You can either: replace the fastpages logo with one of your own choosing, leaving Line 4 as it is; or | add you own file to the images folder and edit the filename in Line 4. | . . The image must be in the **images** folder (or subfolders) for your blog to render correctly. | &lt;/div&gt; . Lines 6-14: Everything here can be removed and replaced with content of your choosing. From my experience, the content can be in html format or markdown format or a mix of both formats. | README.md . Theis file is responsible for the content of the blog’s github repository page – after the folders and files have been listed. . . Two sets of changes can be made to this file: . Line 6: You can totally remove that line, thus removing the View Demo Site badge. On the other hand, you might want to change it to one of your choosing and make it point to another link. . This is quite simple. The shields.io developer site contains example code on how to create your own custom badges. . If you want to use a shield logo other than the ones available, you need to convert your png to base64 code. You can’t just specify a path to the .png file. b64.io is a free resource for converting images to base64 format. The logo is then specified in the badge code using the snippet below. . ?logo=data:image/png;base64,… . You can consult the answers provided to this stackoverflow question if you have any difficulty creating you custom badge. You can create any number of badge this way, all with different attributes. . | Lines 7-554: Everything here can be removed and replaced with content of your choosing. The content has to be in markdown format. . | _config.yml . This is the main configuration file for the blog. This file should be edited carefully as any mistakes will cause errors when GutHub tries to build your blog. The main fastpages README.md provides detailed information on what parts of the blog the parameters in this file control. . However, since we’re talking about simple customizations, here are some: . Line 9: This controls the title of the blog found at the top of the homepage. It is freely customizable. | Line 10: This controls the blog description at the very bottom of the homepage. It is freely customizable. | Lines 39-43: Here you can add links you your social media accounts and other professional accounts. This link provides the list if supported social networks and their keys. | Line 52: This controls the display image previews on home page, for posts that have them. It is set to false by default. You should try it out, it’s a good look. | images . This folder contains images necessary for customizing the look of the blog’s URL. There are a couple of changes you can make here. . logo.png: As stated earlier, this specfies the image preview can be automatically shown with your URL on social media sites. This is freely ccustomizable. | favicon.ico: This controls the page icon shown on the browser tab when the site is opened in browser. There are lots of free resources for downloading .ico file or converting your .png to .ico. . . From my experience, the file name must be **favicon.ico**. | . &lt;/div&gt; . pages/about.md . This file is responsible for the the content of the blog’s About Me page – found on right-side of the top bar. The picture below shows the code and the result on the blog. . . Everything from Lines 7-12 can be removed and replaced with content of your choosing. The content has to be in markdown format. . That’s about it for simple ccustomizations’ you can make your blog at first go. . . Further Customizations . Here are some other advanced customizations you can carry out. They are advanced in the sense that you need to go through the documentation of the various resources used by fastpages in order to figure out to to modify them. . At this point, you’ll be diverging greatly from the default fastpages setup for your blog, thus you’ll need to modify/add multiple files to successfully make a modification. . . Further Links . Here are some useful links the I coudn’t add in the main post: . | | | | . . . .",
            "url": "https://deepwilson.github.io/repository/drafts/Draft-Setting-Up-This-Blog/",
            "relUrl": "/drafts/Draft-Setting-Up-This-Blog/",
            "date": " • Jun 14, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![]()../images/ipynb/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . For the default minima theme: . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For the jekyll massively theme: . Here is a footnote reference [^n], and another [^n]. [^n]: Here is the first footnote [^n]: Here is the second footnote . Here is a footnote reference 1, and another 2. . . Here is the first footnote&#8617; . | Here is the second footnote&#8617; . |",
            "url": "https://deepwilson.github.io/repository/test/",
            "relUrl": "/test/",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . Do a thing . do_thing() Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://deepwilson.github.io/repository/drafts/test-markdown-post/",
            "relUrl": "/drafts/test-markdown-post/",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". My name is Oluwaleke Umar Yusuf and I&#39;m a mechanical engineer. I&#39;m a highly resourceful mechanical engineer with over three years of industry experience in construction and telecommunications. I&#39;m from Nigeria, living in Egypt and currently studying Robotics, Control and Smart Systems at the American University In Cairo. You can find me on GitHub . On this page you can find some information about me and my blog. My blog posts ate mainly geared towards detail my experiences and development with Machine Learning, Computer Vision and Gesture Recognition as I work on my masters. However, there are also posts about topics that I&#39;m most interested in and my opinions or reviews of books I&#39;ve read. . . . Passionate about... . Books | Coding | Music | Poetry | &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . Learning &amp; Reading &amp; Listening . These are books I&#39;m currently reading, courses/tutorials I&#39;m currently taking and music I&#39;m currently listening to. . Currently Learning: . Currently Reading: . --&gt; &lt;/div&gt; . &lt;/div&gt; .",
          "url": "https://deepwilson.github.io/repository/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "Blog",
          "content": "",
          "url": "https://deepwilson.github.io/repository/blog/",
          "relUrl": "/blog/",
          "date": ""
      }
      
  

  

  
      ,"page4": {
          "title": "My Favourites",
          "content": ". Music Blue Neighbourhood Troye Sivan Pop · 2015 . Kamikaze Eminem Hip-Hop/Rap · 2018 . No.6 Collaborations Project Ed Sheeran Pop · 2019 . The 20/20 Experience Justin Timberlake Pop · 2019 . BURDEN BONES Hip-Hop/Rap · 2021 . All Eyez On Me 2Pac Hip-Hop/Rap · 1996 . Dopamine BØRNS Alternative · 2015 . Live at the Royal Albert Hall Bring Me The Horizon Rock · 2020 . Tha Carter IV Lil Wayne Hip-Hop/Rap · 2020 . The Infamous Mobb Deep Hip-Hop/Rap · 1995 . Truth Is a Beautiful Thing London Grammar Alternative · 2017 . 88GLAM2.5 88GLAM Hip-Hop/Rap · 2019 . Beauty Behind the Madness The Weeknd R&amp;B/Soul · 2015 . Stop Staring at the Shadows $uicideboy$ Hip-Hop/Rap · 2020 . Blurryface twenty one pilots Rock · 2015 . 1000 Forms Of Fear Sia Pop · 2014 . . Books . Movies . Podcasts Reflecting History Reflecting History . Making Sense with Sam Harris Sam Harris . The Jordan B. Peterson Podcast Dr. Jordan B. Peterson . Epsilon Theory Podcast Ben Hunt . Naval Naval Ravikant . . Articles Professor Scott Galloway - The Great Grift . Paul Ford - What Is Code? If You Don&#39;t Know, You Need to Read This . The Last Psychiatrist - Who&#39;s Afraid Of Lil Wayne? . Scott Galloway - The Algebra of Wealth . NASA - The Tyranny of the Rocket Equation . Ken Liu - Paper Menagerie . Hunter S. Thompson - Letter on Finding Your Purpose and Living a Meaningful Life . Hans Christian Andersen - Kejserens Nye Klæder (The Emperor&#39;s New Clothes) . David Foster Wallace - This is Water . Ted Chiang - The Merchant and the Alchemist&#39;s Gate . Paul Graham - What You Can&#39;t Say . . Quotes “Once is happenstance. Twice is coincidence. Three times is enemy action.” . ― Ian Fleming . “Tradition is a set of solutions for which we have forgotten the problems” . ― Donald Kingsbury . “People fear death even more than pain. It&#39;s strange that they fear death. Life hurts a lot more than death. At the point of death, the pain is over. Yeah, I guess it is a friend.” . ― Jim Morrison . “There is something profoundly cynical, my friends, in the notion of paradise after death. The lure is evasion. The promise is excusative. One need not accept responsibility for the world as it is, and by extension, one need do nothing about it. To strive for change, for true goodness in this mortal world, one must acknowledge and accept, within one&#39;s own soul, that this mortal reality has purpose in itself, that its greatest value is not for us, but for our children and their children. To view life as but a quick passage along a foul, tortured path - made foul and tortured by our own indifference - is to excuse all manner of misery and depravity, and to exact cruel punishment upon the innocent lives to come.I defy this notion of paradise beyond the gates of bone. If the soul truly survives the passage, then it behooves us - each of us, my friends - to nurture a faith in similitude: what awaits us is a reflection of what we leave behind, and in the squandering of our mortal existence, we surrender the opportunity to learn the ways of goodness, the practice of sympathy, empathy, compassion and healing - all passed by in our rush to arrive at a place of glory and beauty, a place we did not earn, and most certainly do not deserve.”The Apocryphal Teachings of Tanno Spiritwalker Kimloc; The Decade in Ehrlitan . ― Steven Erikson, The Bonehunters . “If you&#39;re harmless, you&#39;re not virtuous. You&#39;re just harmless. You&#39;re like a rabbit. A rabbit isn&#39;t virtuous. It can&#39;t do anything but get eaten.If you&#39;re a monster and you don&#39;t act monstrous, then you&#39;re virtuous. ” . ― Jordan B. Peterson . “Pity those who want the perfect life! Break me, tear me apart, let me be beaten and altered by life, let my life be touched and touch in return for I will be Human, for there is no reason on earth to be scared of yourself.Perfect is boring... let it be written in the sky, painted on buildings, read with your eyes, transcribed on your soul.” . ― Unknown . “Just because fate has chosen something for you instead of you choosing it for yourself doesn’t mean it has to be bad. Even if it’s something you are sure you would never have chosen in a hundred years.” . ― Robert Jordan, The Dragon Reborn . “The Wheel of Time turns, and ages come and pass, leaving memories that become legend. Legend fades to myth, and even myth is long forgotten when the Age that gave it birth comes again. In one Age, called the Third Age by some, an Age yet to come, an Age long past, a wind rose in the Mountains of Mist. The wind was not the beginning. There are neither beginnings nor endings to the turning of the Wheel of time. But it was a beginning.” . ― Robert Jordan, The Eye of the World . “A young wolfhound must meet his first wolf someday, but if the wolf sees him as a puppy, if he acts the puppy, the wolf will surely kill him. The wolfhound must be a wolfhound in the wolf&#39;s eyes even more than in his own, if he is to survive.” . ― Robert Jordan, The Great Hunt . “There are no atheists in foxholes. In the end, everyone prays.” . ― Unknown .",
          "url": "https://deepwilson.github.io/repository/favourites/",
          "relUrl": "/favourites/",
          "date": ""
      }
      
  

  

  
  

  
  

  

  
  

  

  
  

  
  

  
  

  
  

  
  

}