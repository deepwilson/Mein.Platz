{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LFFD Tutorial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNDfaU64Wewmles+8+Ctc3c"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCkNI0oyPWmb"
      },
      "source": [
        "# \"LFFD Paper review\"\n",
        "> \"Analyzing LFFD architecture, a one-stage object detection pipeline to detect faces\"\n",
        "\n",
        "- toc: true\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [deep_learning, resnet]\n",
        "- image: https://images.unsplash.com/photo-1499781350541-7783f6c6a0c8?ixlib=rb-1.2.1&q=85&fm=jpg&crop=entropy&cs=srgb&ixid=eyJhcHBfaWQiOjYzOTIxfQ\n",
        "- hide: false\n",
        "- search_exclude: true\n",
        "- metadata_key1: metadata_value1\n",
        "- metadata_key2: metadata_value2\n",
        "- poem: false\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHETEKp6PKVB"
      },
      "source": [

        "![https://images.unsplash.com/photo-1499781350541-7783f6c6a0c8?ixlib=rb-1.2.1&q=85&fm=jpg&crop=entropy&cs=srgb&ixid=eyJhcHBfaWQiOjYzOTIxfQ](https://images.unsplash.com/photo-1499781350541-7783f6c6a0c8?ixlib=rb-1.2.1&q=85&fm=jpg&crop=entropy&cs=srgb&ixid=eyJhcHBfaWQiOjYzOTIxfQ)\n",
        "\n",
        "# Introduction:\n",
        "\n",
        "---\n",
        "\n",
        "LFFD is an anchor free face detector and belongs to the one-stage category of object detection mechanisms\n",
        "\n",
        "Receptive Fields (RFs) are deployed as natural ‚Äúanchors‚Äù\n",
        "\n",
        "# Contents:\n",
        "\n",
        "---\n",
        "\n",
        "- Receptive Field (RF) vs Effective Receptive Field (ERF)\n",
        "- Large faces vs tiny faces\n",
        "- RFs as Natural \"Anchor\"\n",
        "- Network Architecture\n",
        "\n",
        "> Unlike in fully connected networks, where the value of each unit depends on the entire input to the network, a unit in convolutional networks only depends on a region of the input.\n",
        "This region in the input is the **receptive field** for that unit.\n",
        "\n",
        "Source: Understanding the Effective Receptive Field in Deep Convolutional Neural Networks\n",
        "\n",
        "‚úèÔ∏è‚úèÔ∏è‚úèÔ∏è\n",
        "\n",
        "# Receptive Field (RF) vs Effective\n",
        "Receptive Field (ERF):\n",
        "\n",
        "---\n",
        "\n",
        "![https://i1.wp.com/syncedreview.com/wp-content/uploads/2017/05/24.png?resize=950%2C485&ssl=1](https://i1.wp.com/syncedreview.com/wp-content/uploads/2017/05/24.png?resize=950%2C485&ssl=1)\n",
        "\n",
        "[Image Source](https://i1.wp.com/syncedreview.com/wp-content/uploads/2017/05/24.png?resize=950%2C485&ssl=1)\n",
        "\n",
        "### Receptive field (RF)\n",
        "\n",
        "---\n",
        "\n",
        "Consider the scenario above: (visualisation on lower left) üí°üí°üí°\n",
        "\n",
        "1. Input image (blue üíô) with padding 1 (refer visualisation on upper left)\n",
        "2. 1st convolutional filter with kernel size 3x3 and stride 2 (shaded portion on input image - refer the green üíö lines projected from feature map to input image)\n",
        "3. 2nd convolutional filter with kernel size 3x3 and stride 2 (shaded portion on input image - refer the orange üß° lines projected from feature map to input image)yellow\n",
        "4. First feature map (green üíö) with output size 3x3 ***as a result of convolving first cnn filter with input image***\n",
        "5. Second feature map (orange üß°)  with output size 2x2 ***as a result of convolving second cnn filter with first feature map***\n",
        "6. The size of feature maps reduce consecutively due to stride 2 üößüößüößüöß\n",
        "\n",
        "> If you notice, the receptive field (**RF**) of the first neuron belonging to the first convolutional filter (green üíö) is **** a 3x3 patch of the input image(blue üíô). The output feature map consists of 9 neurons/features (*each neuron will have a RF of 3x3*)\n",
        "\n",
        "> Similarly,  the receptive field (**RF**) of the first neuron belonging to the second convolutional filter (orange üß°)  is **** a 7x7 patch of the input image(blue üíô). The output feature map consists of 4 neurons/features (*each neuron will have a RF of 7x7*)\n",
        "\n",
        "> **With this intuition we can state that the neurons in shallow layers have small RFs and those\n",
        "in deeper layers have large RFs.**\n",
        "\n",
        "üìî For mathematical equations you could refer to following [link](https://syncedreview.com/2017/05/11/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks/)\n",
        "\n",
        "### Effective Receptive field (ERF)\n",
        "\n",
        "---\n",
        "\n",
        "However, one thing that the authors (Luo, Wenjie et al) of the paper *\"Understanding the Effective Receptive Field in Deep Convolutional Neural Networks\"* notice is that not all pixels in the receptive field contribute equally to the output unit‚Äôs response\n",
        "\n",
        "The pixels that actually contribute to learning is what we call the *Effective* *Receptive Field (ERF)*\n",
        "\n",
        "If you notice this is clearly visible by the pixels that are repeatedly covered by the kernel as it is convolved with the input image. The edge pixels are convolved lesser number of times as compared to the centre pixels. (You could try this out in your head and it will make more sense )\n",
        "\n",
        "Thus the ERF plays an important role when it comes to learning.\n",
        "\n",
        "<!-- ![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1cad073b-06ab-4de9-8b5a-ed24d7cb7249/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1cad073b-06ab-4de9-8b5a-ed24d7cb7249/Untitled.png) -->\n",
        "\n",
        "![Image Source](https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11042-020-09143-7/MediaObjects/11042_2020_9143_Fig1_HTML.png)\n",
        "\n",
        "‚úèÔ∏è‚úèÔ∏è‚úèÔ∏è\n",
        "\n",
        "# Large faces vs tiny faces\n",
        "\n",
        "---\n",
        "\n",
        "The paper treats large and tiny faces differently.\n",
        "\n",
        "The assumption is that tiny faces are more difficult to recognize as compared to larger faces.\n",
        "\n",
        "In order to capture tiny faces more contextual information is used to come to a conclusion\n",
        "\n",
        "> As the paper states:\n",
        "\n",
        "- for tiny/small faces, ERFs have to cover the faces as\n",
        "well as sufficient context information\n",
        "- for medium faces, ERFs only have to contain the faces\n",
        "with little context information\n",
        "- for large faces, only keeping them in RFs is enough\n",
        "\n",
        "‚úèÔ∏è‚úèÔ∏è‚úèÔ∏è\n",
        "\n",
        "# RFs as Natural \"Anchor\"\n",
        "\n",
        "---\n",
        "\n",
        "Face detection falls under the category of object detection and the authors try to avoid the use of anchor boxes as in general object detection pipelines and use the receptive fields as natural anchors.\n",
        "\n",
        "Since faces have an aspect ratio of 1:1 kernels with width and height as 1 is used to generate square shaped RFs to detect faces. For the matching strategy, the RF box is matched to a ground truth (GT) box iff its center falls in the GT bbox, other than thresholding IOU\n",
        "\n",
        "‚úèÔ∏è‚úèÔ∏è‚úèÔ∏è       \n",
        "\n",
        "# Network Architecture\n",
        "\n",
        "---\n",
        "\n",
        "![https://img.techpowerup.org/200917/screenshot-from-2020-09-17-11-11-00.png](https://img.techpowerup.org/200917/screenshot-from-2020-09-17-11-11-00.png)\n",
        "\n",
        "The network architecture is divided into 4 parts.\n",
        "\n",
        "- tiny part - has 10 convolution layers\n",
        "- small part\n",
        "- medium part\n",
        "- large part\n",
        "\n",
        "The first two layers downsample the input with stride 4, stride 2 from each\n",
        "\n",
        "# Sources:\n",
        "\n",
        "---\n",
        "\n",
        "- [https://www.youtube.com/watch?v=rj9j_FTI25o](https://www.youtube.com/watch?v=rj9j_FTI25o)\n",
        "- [https://syncedreview.com/2017/05/11/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks/](https://syncedreview.com/2017/05/11/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks/)\n",
        "- [https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285)\n",
        "\n",
        "# Code (under construction):\n",
        "\n",
        "---\n",
        "\n",
        "- [ ]  Data Augmentation\n",
        "- [ ]  LFFD backbone CNN\n",
        "- [ ]  Priors and matching strategy\n",
        "- [ ]  LFFD Loss\n",
        "\n",
        "## Implementation:\n",
        "\n",
        "---\n",
        "\n",
        "The LFFD Loss function is similar to an SSD Multibox Loss function.\n",
        "\n",
        "It consists of :\n",
        "\n",
        "- negative_positive ratio : During the computation, by default most of the bboxes would belong to class 'background'. In order to avoid this imbalance, we keep a limit on the num of background bboxes that would be a part of the loss function (Try a value b/w 3 and 10)\n",
        "- \n",
        "-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}
