{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"kNN & Linear Regression From Scratch: Iris Dataset\"\n",
    "> A step-by-step implementation of the k-Nearest Neighbours and Linear Regression algorithms using the standard Python libaries.\n",
    "\n",
    "- toc: true\n",
    "- author: Oluwaleke Umar Yusuf\n",
    "- badges: true\n",
    "- comments: true\n",
    "- image: images/ipynb/iris_dataset.png\n",
    "- hide: false\n",
    "- search_exclude: false\n",
    "- categories: [notebook]\n",
    "- tags: [kNN, Iris Dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "  .klrid-images-style {\n",
    "    display: flex;\n",
    "    justify-content: center;\n",
    "    align-content: stretch;\n",
    "    flex-wrap: wrap;\n",
    "    flex-direction: row;\n",
    "    text-decoration: none !important;\n",
    "  }\n",
    "  .klrid-images-style img {\n",
    "    margin-right: 5px;\n",
    "    margin-left: 5px;\n",
    "    margin-bottom: 10px;\n",
    "  }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN & Linear Regression From Scratch: Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: You can skip over the **Introduction** section. Jump to **[k-Nearest Neighbours implementation](#k-Nearest-Neighbours-From-Scratch)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) is perhaps the best known database to be found in the pattern recognition literature.\n",
    "\n",
    "The data set (available [here](https://www.kaggle.com/uciml/iris)) consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. **One class (specie) is linearly separable from the other 2; the latter are NOT linearly separable from each other, as seen in the plots below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"klrid-images-style\">\n",
    "\n",
    "  <a><img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_dataset_001.png\" title=\"Credit: https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\" style=\"height:300px\" ></a>\n",
    "  <a><img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_dataset_002.png\" title=\"Credit: https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\" style=\"height:300px\"></a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbours Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [k-Nearest Neighbours](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) algorithm is a non-parametric classification method used for classification and regression. In kNN classification, an object is assigned to the class most common among its `k` nearest neighbors (k is a positive integer, typically small). If `k = 1`, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "\n",
    "![k-Nearest Neighbours](https://i.ibb.co/VQMh7p4/knn.png \"Credit: http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final1_ibdm8a.png\")&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Jump to **[k-Nearest Neighbours implementation](#k-Nearest-Neighbours-From-Scratch)**. The raw notebook file can be found [here](https://github.com/Outsiders17711/Mein.Platz/blob/main/_notebooks/ipynb_data/kNN-Linear-Regression-Iris_Dataset.ipynb).\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Linear Regression](https://en.wikipedia.org/wiki/Linear_regression) algorithm is a linear approach to modelling the relationship between a scalar response `(y — dependent variables)` and one or more explanatory variables `(X — independent variables)`. Linear Regression fits a linear model with coefficients `w = (w1, w2, … , wn)` to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://wiki.aalto.fi/download/attachments/155464048/linreg.png?version=1&modificationDate=1565612055936&api=v2\" title=\"Credit: https://wiki.aalto.fi/display/ITSP/Linear+regression\" style=\"height:300px;\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Jump to **[Linear Regression implementation](#Linear-Regression-From-Scratch)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# importing required libraries\n",
    "import random\n",
    "import csv\n",
    "import math\n",
    "import statistics\n",
    "import copy\n",
    "\n",
    "# set random seed\n",
    "random.seed('iris dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbours From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flowchart for implementing the kNN algorithm is shown below. Each step in the implementation will be wrapped in its own function for clarity.\n",
    "\n",
    "![kNN Flowchart](https://raw.githubusercontent.com/Outsiders17711/Mein.Platz/main/images/ipynb/knn_flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is contained in a .csv file. We will implement a function `DataLoader` that calls several child functions to load and cleanup the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "We start with the `_load_csv()` function which will use the python [csv](https://docs.python.org/3/library/csv.html) module to read the file contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['5.1', '3.5', '1.4', '0.2', 'Iris-setosa'],\n",
       " ['4.9', '3.0', '1.4', '0.2', 'Iris-setosa'],\n",
       " ['6.4', '3.2', '4.5', '1.5', 'Iris-versicolor'],\n",
       " ['6.9', '3.1', '4.9', '1.5', 'Iris-versicolor'],\n",
       " ['6.2', '3.4', '5.4', '2.3', 'Iris-virginica'],\n",
       " ['5.9', '3.0', '5.1', '1.8', 'Iris-virginica']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "def _load_csv(filename):\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = csv.reader(file)\n",
    "\t\treturn [row for row in csv_reader if row]\n",
    "\n",
    "dataset = _load_csv(\"ipynb_data/IrisData.csv\")\n",
    "dataset[:2]+dataset[51:53]+dataset[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the csv reader loads the entire dataset as a list of list, each inner list containing `['sepal length', 'sepal width', 'petal length', 'petal width', 'species']`. We need to convert the four features from strings to floats. Let's create a function called `_clean_features()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5.1, 3.5, 1.4, 0.2, 'Iris-setosa'],\n",
       " [4.9, 3.0, 1.4, 0.2, 'Iris-setosa'],\n",
       " [6.4, 3.2, 4.5, 1.5, 'Iris-versicolor'],\n",
       " [6.9, 3.1, 4.9, 1.5, 'Iris-versicolor'],\n",
       " [6.2, 3.4, 5.4, 2.3, 'Iris-virginica'],\n",
       " [5.9, 3.0, 5.1, 1.8, 'Iris-virginica']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "def _clean_features(dataset):\n",
    "    num_columns = len(dataset[0])\n",
    "\n",
    "    for row in dataset:\n",
    "        for column in range(num_columns-1):\n",
    "            row[column] = float(row[column].strip())\n",
    "\n",
    "_clean_features(dataset)\n",
    "dataset[:2]+dataset[51:53]+dataset[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, in machine learning, it is preferred that all data be numeric (floats or integers). Thus, we need convert each unique class values (species) to integers and create a map between the integer values and the actual string values. We'll create a function called `_map_classes()` to acheive this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[5.1, 3.5, 1.4, 0.2, 0],\n",
       " [4.9, 3.0, 1.4, 0.2, 0],\n",
       " [6.4, 3.2, 4.5, 1.5, 1],\n",
       " [6.9, 3.1, 4.9, 1.5, 1],\n",
       " [6.2, 3.4, 5.4, 2.3, 2],\n",
       " [5.9, 3.0, 5.1, 1.8, 2]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "def _map_classes(dataset):\n",
    "    class_mappings = {}\n",
    "    for row in dataset:\n",
    "        _specie = row[-1]\n",
    "        if _specie not in class_mappings.keys():\n",
    "            class_mappings[_specie] = len(class_mappings)\n",
    "        row[-1] = class_mappings[_specie]\n",
    "\n",
    "    return class_mappings\n",
    "\n",
    "class_mappings = _map_classes(dataset)\n",
    "print(class_mappings)\n",
    "dataset[:2]+dataset[51:53]+dataset[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Now we have successfully cleaned out data and mapped the classes.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Machine Learning, two data scaling methods are most commonly discussed: [Normalization](\"https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization)\") and [Standardization](\"https://en.wikipedia.org/wiki/Feature_scaling#Standardization_(Z-score_Normalization)\").\n",
    "\n",
    "Normalization typically means rescaling the data into a range of [0,1]. Standardization typically means rescaling the data to have a mean of 0 and a standard deviation of 1 (unit variance). This [article](https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf) analyzes the empirical results of applying different scaling methods on features in multiple experiments settings.\n",
    "\n",
    "Distance-based algorithms (such as kNN) are affected by the scale of the variables and will give higher weightage to variables which have higher magnitude. To prevent the algorithm from being biased towards variables with higher magnitude, we can bring down all the variables to the same scale.\n",
    "\n",
    "In our case, we will make use of Normalization (also known as min-max scaling) as it limits the range of the data and thus, a better option for kNN. Let's create a function called `_normalize_data()`, based on the Normalization formula shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://miro.medium.com/max/1400/1*GwcC80mDUyscDAWtwZSBdA.png\" title=\"Credit: https://www.kdnuggets.com/2020/04/data-transformation-standardization-normalization.html\" style=\"height:150px\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.22222222222222213,\n",
       "  0.6249999999999999,\n",
       "  0.06779661016949151,\n",
       "  0.04166666666666667,\n",
       "  0],\n",
       " [0.1666666666666668,\n",
       "  0.41666666666666663,\n",
       "  0.06779661016949151,\n",
       "  0.04166666666666667,\n",
       "  0],\n",
       " [0.5833333333333334, 0.5, 0.5932203389830508, 0.5833333333333334, 1],\n",
       " [0.7222222222222222,\n",
       "  0.4583333333333333,\n",
       "  0.6610169491525424,\n",
       "  0.5833333333333334,\n",
       "  1],\n",
       " [0.5277777777777778,\n",
       "  0.5833333333333333,\n",
       "  0.7457627118644068,\n",
       "  0.9166666666666666,\n",
       "  2],\n",
       " [0.44444444444444453,\n",
       "  0.41666666666666663,\n",
       "  0.6949152542372881,\n",
       "  0.7083333333333334,\n",
       "  2]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "def _normalize_data(dataset):\n",
    "    num_features = len(dataset[0])-1\n",
    "    for i in range(num_features):\n",
    "        column_values = [row[i] for row in dataset]\n",
    "        column_min = min(column_values)\n",
    "        column_max = max(column_values)\n",
    "        \n",
    "        for row in dataset:\n",
    "            row[i] = (row[i] - column_min) / (column_max - column_min)\n",
    "\n",
    "_normalize_data(dataset)\n",
    "dataset[:2]+dataset[51:53]+dataset[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Important: Now putting all the child functions together in the main function:\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataLoader(filename):\n",
    "    dataset = _load_csv(filename)\n",
    "    _clean_features(dataset)\n",
    "    class_mappings = _map_classes(dataset)\n",
    "    _normalize_data(dataset)\n",
    "\n",
    "    return dataset, class_mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're done preparing the dataset for the classification using the kNN algorithm. Next, we implement the algorithm itself in a main function `kNN_Algorithm` that calls several child functions.  \n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Euclidean Distances "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a function that calculates the distance between two sets of data features. There are different distance formulas available but the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) is most commonly used for kNN classification problems.  \n",
    "\n",
    "In mathematics, the Euclidean distance between two points in Euclidean space is the length of a line segment between the two points. The formula is shown below and is implemented with the `_euclidean_distance()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Euclidean Distance Formula](https://aws1.discourse-cdn.com/codecademy/original/5X/e/2/9/6/e296139537a35d53394033117a05f12a401d4c42.png \"Credit: https://discuss.codecademy.com/t/faq-distance-formula-euclidean-distance/374303\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.21561353744805575, 0.8458718030210156, 0.9646282869629299)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "def _euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    num_features = len(row1)-1\n",
    "\n",
    "    for i in range(num_features):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    return math.sqrt(distance)\n",
    "\n",
    "(\n",
    "    _euclidean_distance(dataset[0], dataset[1]), # class 0 VS class 0\n",
    "    _euclidean_distance(dataset[0], dataset[51]), # class 0 VS class 1\n",
    "    _euclidean_distance(dataset[0], dataset[-1]) # class 0 VS class 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get k Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to get the k nearest neighbours of a given (test) row (set of features) amongst a larger sample of (training) rows (sets of features). What we do is simply calculate the distances between the test row and all the training rows to get the `k` training rows with the smallest Euclidean distances. Let's create a function `get_k_neighbours()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.22222222222222213,\n",
       "  0.6249999999999999,\n",
       "  0.06779661016949151,\n",
       "  0.04166666666666667,\n",
       "  0],\n",
       " [0.1666666666666668,\n",
       "  0.41666666666666663,\n",
       "  0.06779661016949151,\n",
       "  0.04166666666666667,\n",
       "  0],\n",
       " [0.5833333333333334, 0.5, 0.5932203389830508, 0.5833333333333334, 1],\n",
       " [0.7222222222222222,\n",
       "  0.4583333333333333,\n",
       "  0.6610169491525424,\n",
       "  0.5833333333333334,\n",
       "  1],\n",
       " [0.44444444444444453,\n",
       "  0.41666666666666663,\n",
       "  0.6949152542372881,\n",
       "  0.7083333333333334,\n",
       "  2],\n",
       " [0.5277777777777778,\n",
       "  0.5833333333333333,\n",
       "  0.7457627118644068,\n",
       "  0.9166666666666666,\n",
       "  2]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "def _get_k_neighbours(test_row, train_data, num_neighbours):\n",
    "    test_train_distances = []\n",
    "    for train_row in train_data:\n",
    "        _test_train_distance = _euclidean_distance(test_row, train_row)\n",
    "        test_train_distances.append([train_row, _test_train_distance])\n",
    "\n",
    "    test_train_distances.sort(key=lambda idx: idx[1])\n",
    "    return [test_train_distances[i][0] for i in range(num_neighbours)]\n",
    "\n",
    "_get_k_neighbours(dataset[0], dataset[:2]+dataset[51:53]+dataset[-2:], num_neighbours=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we predict the class of the test row based on the most occuring class amongst it's k nearest neighbours. We'll create a function called `_predict_classification()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-output\n",
    "def _predict_classification(test_row, train_data, num_neighbours):\n",
    "    nearest_neighbours =  _get_k_neighbours(test_row, train_data, num_neighbours)\n",
    "    nearest_classes = [neighbour[-1] for neighbour in nearest_neighbours]\n",
    "    predicted_class = max(set(nearest_classes), key=nearest_classes.count)\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "_predict_classification(dataset[0], dataset[:2]+dataset[51:53]+dataset[-2:], num_neighbours=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Important: Now putting all the child functions together in the main function:\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN_Algorithm(test_data, train_data, num_neighbours):\n",
    "    return [_predict_classification(test_row, train_data, num_neighbours) for test_row in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can go ahead and evaluate the performance of the algorithm against the dataset. The evaluation will be implemented using the function `Evaluate_kNN_Algorithm` which calls several child functions to split the dataset into test/train samples and calculate accuracies.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset Into Training & Testing Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try two popular methods for splitting the dataset in training and testing samples:\n",
    "\n",
    "- *Test/Train Split*: The dataset is shuffled and a percentage is used for training and the rest for testing. The algorithm is then trained on the training sample and it's performance evaluated using the testing sample. This is implemented in the `_test_train_split()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/07/15185319/blogs-15-7-2020-02-1024x565.jpg\" title=\"Credit: https://www.analyticsvidhya.com/blog/2021/05/importance-of-cross-validation-are-evaluation-metrics-enough/\" style=\"height:300px;\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test_train_split(dataset, test_ratio):\n",
    "    _dataset = copy.deepcopy(dataset)\n",
    "    random.shuffle(_dataset)\n",
    "\n",
    "    split_index = int(len(dataset) * test_ratio)\n",
    "    # Training data\n",
    "    test_sample = _dataset[0:split_index]\n",
    "    #Testing data\n",
    "    train_sample = _dataset[split_index:]\n",
    "\n",
    "    return test_sample, train_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Cross Validation Split*: The dataset is shuffled and split into k groups. The algorithm is then trained and evaluated k times and the performance summarized by taking the mean performance score. During each training and evaluation step, one of the k groups is used as the testing sample and the remaining groups as the training sample.  This is implemented in the `_cross_validation_split()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a><img src=\"https://zitaoshen.rbind.io/project/machine_learning/machine-learning-101-cross-vaildation/featured.png\" title=\"Credit: https://www.analyticsvidhya.com/blog/2021/05/importance-of-cross-validation-are-evaluation-metrics-enough/\" style=\"height:300px;\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cross_validation_split(dataset, num_groups):\n",
    "    dataset_groups = []\n",
    "    _dataset = copy.deepcopy(dataset)\n",
    "    group_size = int(len(_dataset) / num_groups)\n",
    "\n",
    "    for i in range(num_groups):\n",
    "        group = []\n",
    "        while len(group) < group_size:\n",
    "            idx = random.randrange(len(_dataset))\n",
    "            group.append(_dataset.pop(idx))\n",
    "        dataset_groups.append(group)\n",
    "\n",
    "    return dataset_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Accuracy Metric "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a simple function `_get_accuracy()` that returns the percentage of the test classes correctly predicted by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_accuracy(test_sample, algorithm_predictions, class_mappings):\n",
    "    test_classes = [row[-1] for row in test_sample]\n",
    "    num_test_classes = len(test_classes)\n",
    "    test_labels = list(class_mappings.keys())\n",
    "\n",
    "    if len(test_classes) != len(algorithm_predictions):\n",
    "        raise IndexError(\"The count of test classes is not equal to the count of algorithm predictions!\")\n",
    "\n",
    "    num_correct_predictions = sum([actual == predicted for actual, predicted \n",
    "                                                        in zip(test_classes, algorithm_predictions)])\n",
    "\n",
    "    wrong_predictions = [f'A:{test_labels[actual]} | P:{test_labels[predicted]}'\n",
    "                                                            for actual, predicted \n",
    "                                                            in zip(test_classes, algorithm_predictions)\n",
    "                                                            if actual != predicted]\n",
    "                        \n",
    "    accuracy = (num_correct_predictions / num_test_classes) * 100\n",
    "    return accuracy, wrong_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the different manners in which Test/Train Split & Cross Validation Split divide the dataset, we will create two separate functions for the final algorithm evaluation:\n",
    "\n",
    "1.  `tts_Evaluate_kNN_Algorithm`: kNN algorithm evaluation using the Test/Train Split method; and\n",
    "\n",
    "2.  `cvs_Evaluate_kNN_Algorithm`: kNN algorithm evaluation using the Cross Validation Split method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tts_Evaluate_kNN_Algorithm(dataset, class_mappings, test_ratio=0.25, \n",
    "                                                                num_neighbours=3, num_iterations=100):\n",
    "    \n",
    "    ACCURACY_HISTORY = []\n",
    "    WRONG_PREDICTION_HISTORY = []\n",
    "\n",
    "    for _iter in range(num_iterations):\n",
    "        _dataset = copy.deepcopy(dataset)\n",
    "        test_sample, train_sample = _test_train_split(_dataset, test_ratio)\n",
    "\n",
    "        algorithm_predictions = kNN_Algorithm(test_sample, train_sample, num_neighbours)\n",
    "        accuracy, wrong_predictions = _get_accuracy(test_sample, algorithm_predictions, class_mappings)\n",
    "        ACCURACY_HISTORY.append(accuracy)\n",
    "        WRONG_PREDICTION_HISTORY.extend(wrong_predictions)\n",
    "\n",
    "    random.shuffle(WRONG_PREDICTION_HISTORY)\n",
    "    print('kNN algorithm evaluation using the Test/Train Split method:', '\\n\\t', \n",
    "                'Average Accuracy:', round(statistics.mean(ACCURACY_HISTORY), ndigits=4), '\\n\\t', \n",
    "                'Maximum Accuracy:', max(ACCURACY_HISTORY), '\\n')\n",
    "\n",
    "    print('A: Actual | P: Predicted')\n",
    "    print('\\n'.join(WRONG_PREDICTION_HISTORY[:20]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN algorithm evaluation using the Test/Train Split method: \n",
      "\t Average Accuracy: 94.8649 \n",
      "\t Maximum Accuracy: 100.0 \n",
      "\n",
      "A: Actual | P: Predicted\n",
      "A:Iris-versicolor | P:Iris-virginica\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-versicolor | P:Iris-virginica\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-versicolor | P:Iris-virginica\n",
      "A:Iris-versicolor | P:Iris-virginica\n",
      "A:Iris-versicolor | P:Iris-virginica\n",
      "A:Iris-versicolor | P:Iris-virginica\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-virginica | P:Iris-versicolor\n"
     ]
    }
   ],
   "source": [
    "#collapse-output\n",
    "dataset, class_mappings = DataLoader(\"ipynb_data/IrisData.csv\")\n",
    "tts_Evaluate_kNN_Algorithm(dataset, class_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvs_Evaluate_kNN_Algorithm(dataset, class_mappings, num_groups=5, \n",
    "                                                                num_neighbours=3, num_iterations=100):\n",
    "    \n",
    "    ACCURACY_HISTORY = []\n",
    "    WRONG_PREDICTION_HISTORY = []\n",
    "\n",
    "    for _iter in range(num_iterations):\n",
    "        _dataset = copy.deepcopy(dataset)\n",
    "        dataset_groups = _cross_validation_split(_dataset, num_groups)\n",
    "\n",
    "        for idx, group in enumerate(dataset_groups):\n",
    "            test_sample = group\n",
    "            _train_sample = copy.deepcopy(dataset_groups)\n",
    "            del _train_sample[idx]\n",
    "            \n",
    "            train_sample = []\n",
    "            for train_group in _train_sample:\n",
    "                train_sample.extend(train_group)\n",
    "\n",
    "            algorithm_predictions = kNN_Algorithm(test_sample, train_sample, num_neighbours)\n",
    "            accuracy, wrong_predictions = _get_accuracy(test_sample, algorithm_predictions, class_mappings)\n",
    "            ACCURACY_HISTORY.append(accuracy)\n",
    "            WRONG_PREDICTION_HISTORY.extend(wrong_predictions)\n",
    "\n",
    "    random.shuffle(WRONG_PREDICTION_HISTORY)\n",
    "    print('kNN algorithm evaluation using the Cross Validation Split method:', '\\n\\t', \n",
    "                'Average Accuracy:', round(statistics.mean(ACCURACY_HISTORY), ndigits=4), '\\n\\t', \n",
    "                'Maximum Accuracy:', max(ACCURACY_HISTORY), '\\n')\n",
    "\n",
    "    print('A: Actual | P: Predicted')\n",
    "    print('\\n'.join(WRONG_PREDICTION_HISTORY[:20]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN algorithm evaluation using the Cross Validation Split method: \n",
      "\t Average Accuracy: 95.22 \n",
      "\t Maximum Accuracy: 100.0 \n",
      "\n",
      "A: Actual | P: Predicted\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-versicolor | P:Iris-virginica\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-versicolor | P:Iris-virginica\n",
      "A:Iris-versicolor | P:Iris-virginica\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-versicolor | P:Iris-virginica\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-versicolor | P:Iris-virginica\n",
      "A:Iris-versicolor | P:Iris-virginica\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-virginica | P:Iris-versicolor\n",
      "A:Iris-versicolor | P:Iris-virginica\n",
      "A:Iris-versicolor | P:Iris-virginica\n",
      "A:Iris-virginica | P:Iris-versicolor\n"
     ]
    }
   ],
   "source": [
    "#collapse-output\n",
    "dataset, class_mappings = DataLoader(\"ipynb_data/IrisData.csv\")\n",
    "cvs_Evaluate_kNN_Algorithm(dataset, class_mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the kNN algorithm evaluation using the Cross Validation Split and Test/Train Split method give similar results: **maximum accuracy of 100%** and an **average accuracy of ~95%** across **100 iterations** using **k=3 neighbours**.\n",
    "\n",
    "A closer look at the randomly printed wrong classifications shows that the algorithm only has porblems differentiating between `Iris-virginica` and `Iris-versicolor`. It has no problem correctly classifying `Iris-setosa`.\n",
    "\n",
    "> Recall: ** Class Mappings = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2} **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms the statement in the Iris Dataset description that:\n",
    "\n",
    "> \"**One class (specie) is linearly separable from the other 2; the latter are NOT linearly separable from each other...**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The raw notebook file for this post \\[`k-Nearest Neighbours From Scratch`\\] can be found [here](https://github.com/Outsiders17711/Mein.Platz/blob/main/_notebooks/ipynb_data/kNN-Linear-Regression-Iris_Dataset.ipynb).\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression From Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be the end of this post. I will create a separate post for **Linear Regression From Scratch** and link it here.\n",
    "\n",
    "Thank you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources & References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  [random — Generate pseudo-random numbers](https://docs.python.org/3/library/random.html)\n",
    "\n",
    "-  [Develop k-Nearest Neighbors in Python From Scratch - Machine Learning Mastery](https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/)\n",
    "\n",
    "-  [K Nearest Neighbors Algorithm using Python From Absolute Scratch - The Nerdy Dev](https://www.youtube.com/watch?v=uclqpQe8TMQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: **[Jump To Top](#kNN-&-Linear-Regression-From-Scratch:-Iris-Dataset)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64ed01536bf604a35b39c8ffb5f8754e0194885370844d2a99bdfd2c5ccd0baa"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
